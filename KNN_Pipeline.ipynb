{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd2e9ad7",
   "metadata": {},
   "source": [
    "# Neccessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b3878c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix\n",
    "from Data_Splitter import split_users, holdout_interactions\n",
    "from Fairness_Metrics import compute_recGap, compute_compounding_factor\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac597e",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291b1aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the data\n",
    "def build_data(df):\n",
    "    # Split users into train/validation/test groups (based solely on user_id)\n",
    "    train_users, val_users, test_users = split_users(df, train_frac=0.6, val_frac=0.2, test_frac=0.2)\n",
    "\n",
    "    # For train users, use all interactions.\n",
    "    df_train_all = df[df['user_id'].isin(train_users)]\n",
    "\n",
    "    # For validation and test users, further split each user's interactions (80% train, 20% holdout)\n",
    "    df_val_all = df[df['user_id'].isin(val_users)]\n",
    "    df_test_all = df[df['user_id'].isin(test_users)]\n",
    "\n",
    "    df_val_train, df_val_holdout = holdout_interactions(df_val_all, holdout_frac=0.2)\n",
    "    df_test_train, df_test_holdout = holdout_interactions(df_test_all, holdout_frac=0.2)\n",
    "\n",
    "    # Build the overall training data by combining all train users and the training portion for validation/test users.\n",
    "    df_model_train = pd.concat([df_train_all, df_val_train, df_test_train]).reset_index(drop=True)\n",
    "\n",
    "    return df_model_train, df_val_holdout, df_test_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d14837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binarized userâ€“track interaction matrix from the training data.\n",
    "def create_user_track_matrix(df_model_train):\n",
    "    user_track_matrix = pd.crosstab(df_model_train['user_id'], df_model_train['track_id'])\n",
    "    user_track_matrix = (user_track_matrix > 0).astype(int)\n",
    "\n",
    "    # For an item-based KNN recommender, we treat each track (item) as a vector of user interactions.\n",
    "    # Transpose the matrix and create a sparse representation (tracks x users).\n",
    "    sparse_item_matrix = csr_matrix(user_track_matrix.T.values)\n",
    "    track_list = list(user_track_matrix.columns)\n",
    "\n",
    "    return user_track_matrix, sparse_item_matrix, track_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce50cbdf",
   "metadata": {},
   "source": [
    "# KNNItem Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c24636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_similarities(knn_model, track_id, sparse_item_matrix, track_list, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Retrieve the n most similar tracks for a given track_id.\n",
    "    \"\"\"\n",
    "    if track_id not in track_list:\n",
    "        return []\n",
    "    track_index = track_list.index(track_id)\n",
    "    track_vector = sparse_item_matrix[track_index]\n",
    "    # Retrieve neighbors (n_neighbors + 1 because the track itself is returned)\n",
    "    distances, indices = knn_model.kneighbors(track_vector, n_neighbors=n_neighbors + 1)\n",
    "    # Skip the first index if it is the track itself.\n",
    "    similar_indices = [i for i in indices.flatten() if i != track_index]\n",
    "    similar_tracks = [track_list[i] for i in similar_indices]\n",
    "    return similar_tracks\n",
    "\n",
    "def get_recommendations_for_user(knn_model, user_id, user_track_matrix, sparse_item_matrix, track_list, top_n=10, n_neighbors=10):\n",
    "    \"\"\"\n",
    "    For a given user, aggregate similar items from the items the user has interacted with in the training data.\n",
    "    Only recommend items the user has not interacted with.\n",
    "    \"\"\"\n",
    "    if user_id not in user_track_matrix.index:\n",
    "        return []\n",
    "    # Get the set of tracks the user has interacted with (training interactions)\n",
    "    user_history = set(user_track_matrix.loc[user_id][lambda row: row == 1].index)\n",
    "    \n",
    "    candidate_scores = {}\n",
    "    # For each item in the user history, get similar items and sum a simple frequency score.\n",
    "    for item in user_history:\n",
    "        similar_items = get_item_similarities(knn_model, item, sparse_item_matrix, track_list, n_neighbors=n_neighbors)\n",
    "        for sim_item in similar_items:\n",
    "            if sim_item in user_history:\n",
    "                continue\n",
    "            candidate_scores[sim_item] = candidate_scores.get(sim_item, 0) + 1\n",
    "                \n",
    "    ranked_items = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    recommended_items = [item for item, score in ranked_items][:top_n]\n",
    "    return recommended_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992f85e",
   "metadata": {},
   "source": [
    "## KNNItem Evaluation Using NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1533ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(relevances, k):\n",
    "    \"\"\"\n",
    "    Compute NDCG@k given a list of binary relevance scores.\n",
    "    \"\"\"\n",
    "    relevances = np.asfarray(relevances)[:k]\n",
    "    if relevances.size == 0:\n",
    "        return 0.0\n",
    "    # Discount factors (log2-based)\n",
    "    discounts = np.log2(np.arange(2, relevances.size + 2))\n",
    "    dcg = np.sum(relevances / discounts)\n",
    "    # Ideal DCG (sorted relevances)\n",
    "    ideal_relevances = np.sort(relevances)[::-1]\n",
    "    idcg = np.sum(ideal_relevances / discounts)\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def evaluate_ndcg(knn_model, df, holdout_df, user_track_matrix, sparse_item_matrix, track_list, top_n=10, n_neighbors=10):\n",
    "    \"\"\"\n",
    "    Evaluate recommendations using NDCG@k for each user in a holdout set.\n",
    "    Returns overall NDCG and NDCG by gender.\n",
    "    \"\"\"\n",
    "    # Create mapping from user_id to their holdout (ground truth) track_ids.\n",
    "    user_holdout = holdout_df.groupby('user_id')['track_id'].apply(set).to_dict()\n",
    "    # Get user genders from the original data (assuming 'gender' column exists).\n",
    "    user_gender = df.set_index('user_id')['gender'].to_dict()\n",
    "    \n",
    "    ndcg_scores = {}    # per user scores\n",
    "    ndcg_by_gender = {} # aggregated scores per gender\n",
    "    \n",
    "    for user, true_items in user_holdout.items():\n",
    "        # Generate recommendations using the training data.\n",
    "        recs = get_recommendations_for_user(knn_model, user, user_track_matrix, sparse_item_matrix, track_list, top_n=top_n, n_neighbors=n_neighbors)\n",
    "        # Binary relevance: 1 if the recommended item is in the holdout set, 0 otherwise.\n",
    "        relevances = [1 if rec in true_items else 0 for rec in recs]\n",
    "        ndcg = ndcg_at_k(relevances, top_n)\n",
    "        ndcg_scores[user] = ndcg\n",
    "        \n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        if gender not in ndcg_by_gender:\n",
    "            ndcg_by_gender[gender] = []\n",
    "        ndcg_by_gender[gender].append(ndcg)\n",
    "    \n",
    "    overall_ndcg = np.mean(list(ndcg_scores.values())) if ndcg_scores else 0.0\n",
    "    avg_ndcg_by_gender = {gender: np.mean(scores) for gender, scores in ndcg_by_gender.items()}\n",
    "\n",
    "    print(\"\\nSet Evaluation:\")\n",
    "    print(f\"Overall NDCG@{top_n}: {overall_ndcg:.4f}\")\n",
    "    print(\"NDCG by gender:\", avg_ndcg_by_gender)\n",
    "\n",
    "    return overall_ndcg, avg_ndcg_by_gender\n",
    "\n",
    "\n",
    "def grid_search_validation(knn_model, user_track_matrix, sparse_item_matrix, track_list, df, val_holdout_df, candidate_neighbors, candidate_top_n):\n",
    "    \"\"\"\n",
    "    Perform grid search over n_neighbors and top_n parameters on the validation holdout set.\n",
    "    Returns the best hyperparameters (those that achieve the highest overall NDCG) and grid search results.\n",
    "    \"\"\"\n",
    "    best_ndcg = -1.0\n",
    "    best_params = None\n",
    "    grid_results = []  # Store tuples: (n_neighbors, top_n, overall_ndcg)\n",
    "    \n",
    "    for n_neighbors_param in candidate_neighbors:\n",
    "        for top_n_param in candidate_top_n:\n",
    "            overall_ndcg_val, _ = evaluate_ndcg(knn_model, df, val_holdout_df, user_track_matrix, sparse_item_matrix, track_list, top_n=top_n_param, n_neighbors=n_neighbors_param)\n",
    "            grid_results.append((n_neighbors_param, top_n_param, overall_ndcg_val))\n",
    "            print(f\"n_neighbors: {n_neighbors_param}, top_n: {top_n_param} => NDCG: {overall_ndcg_val:.4f}\")\n",
    "            if overall_ndcg_val > best_ndcg:\n",
    "                best_ndcg = overall_ndcg_val\n",
    "                best_params = (n_neighbors_param, top_n_param)\n",
    "\n",
    "    print(\"\\nBest hyperparameters (n_neighbors, top_n):\", best_params)\n",
    "    print(\"Best overall NDCG on validation set:\", best_ndcg)\n",
    "    \n",
    "    return best_params, best_ndcg, grid_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b48f70a",
   "metadata": {},
   "source": [
    "# Other Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06336e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diversity_for_list(recommended_tracks, sparse_item_matrix, track_list):\n",
    "    \"\"\"\n",
    "    Compute intra-list diversity: average dissimilarity among all pairs of recommended tracks.\n",
    "    Dissimilarity is defined as (1 - cosine similarity) for each pair.\n",
    "    \"\"\"\n",
    "    if len(recommended_tracks) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Retrieve indices for the recommended tracks from track_list.\n",
    "    indices = [track_list.index(t) for t in recommended_tracks if t in track_list]\n",
    "    \n",
    "    # Extract the corresponding item vectors from the sparse matrix.\n",
    "    vectors = sparse_item_matrix[indices]\n",
    "    \n",
    "    # Compute pairwise cosine similarity.\n",
    "    sim_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    # Compute average pairwise similarity (ignoring the diagonal)\n",
    "    sum_similarity = 0.0\n",
    "    count = 0\n",
    "    n = len(indices)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            sum_similarity += sim_matrix[i, j]\n",
    "            count += 1\n",
    "    \n",
    "    avg_similarity = sum_similarity / count if count > 0 else 0.0\n",
    "    # Diversity is defined as the complement of similarity.\n",
    "    return 1 - avg_similarity\n",
    "\n",
    "def evaluate_metrics(knn_model, df, holdout_df, user_track_matrix, sparse_item_matrix, track_list, top_n=10, n_neighbors=10):\n",
    "    \"\"\"\n",
    "    Evaluate recommendations for all users in the holdout set using Recall@10, Coverage@10, and Diversity@10.\n",
    "    Also, compute the metrics for each gender subgroup.\n",
    "    \n",
    "    Parameters:\n",
    "        knn_model         : The trained KNN model.\n",
    "        holdout_df        : DataFrame with ground-truth interactions (must include 'user_id' and 'track_id').\n",
    "        user_track_matrix : Training user-by-item matrix.\n",
    "        sparse_item_matrix: Sparse representation of item vectors.\n",
    "        track_list        : List of track IDs.\n",
    "        df                : The original DataFrame containing user attributes (e.g., 'gender').\n",
    "        top_n             : Number of recommendations per user.\n",
    "        n_neighbors       : Number of neighbors to consider.\n",
    "        \n",
    "        Returns:\n",
    "        overall_recall, overall_coverage, overall_diversity, and a dictionary `gender_metrics`\n",
    "        that contains per-gender averages for recall, coverage, and diversity.\n",
    "    \"\"\"\n",
    "    # Mapping from user_id to their ground truth track_ids.\n",
    "    user_holdout = holdout_df.groupby('user_id')['track_id'].apply(set).to_dict()\n",
    "    # Mapping from user_id to gender.\n",
    "    user_gender = df.set_index('user_id')['gender'].to_dict()\n",
    "    \n",
    "    recall_scores = {}\n",
    "    diversity_scores = {}\n",
    "    # For coverage per gender, maintain a set of recommended items per gender.\n",
    "    coverage_by_gender = {}\n",
    "    \n",
    "    for user, ground_truth in user_holdout.items():\n",
    "        recs = get_recommendations_for_user(knn_model, user, user_track_matrix, sparse_item_matrix, track_list, top_n=top_n, n_neighbors=n_neighbors)\n",
    "        \n",
    "        # Compute Recall@10.\n",
    "        if ground_truth:\n",
    "            recall = len(set(recs).intersection(ground_truth)) / len(ground_truth)\n",
    "        else:\n",
    "            recall = 0.0\n",
    "        recall_scores[user] = recall\n",
    "        \n",
    "        # Compute Diversity@10.\n",
    "        diversity = compute_diversity_for_list(recs, sparse_item_matrix, track_list)\n",
    "        diversity_scores[user] = diversity\n",
    "        \n",
    "        # Collect recommended items per gender for Coverage.\n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        if gender not in coverage_by_gender:\n",
    "            coverage_by_gender[gender] = set()\n",
    "        coverage_by_gender[gender].update(recs)\n",
    "    \n",
    "    overall_recall = np.mean(list(recall_scores.values()))\n",
    "    overall_diversity = np.mean(list(diversity_scores.values()))\n",
    "    overall_coverage = len(set().union(*(set(recs) for recs in coverage_by_gender.values()))) / len(track_list)\n",
    "    \n",
    "    # Compute per-gender averages.\n",
    "    recall_by_gender = {}\n",
    "    diversity_by_gender = {}\n",
    "    coverage_metrics_by_gender = {}\n",
    "    \n",
    "    # Organize per-user metrics by gender.\n",
    "    for user, rec in recall_scores.items():\n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        if gender not in recall_by_gender:\n",
    "            recall_by_gender[gender] = []\n",
    "        recall_by_gender[gender].append(rec)\n",
    "    \n",
    "    for user, div in diversity_scores.items():\n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        if gender not in diversity_by_gender:\n",
    "            diversity_by_gender[gender] = []\n",
    "        diversity_by_gender[gender].append(div)\n",
    "    \n",
    "    for gender, rec_set in coverage_by_gender.items():\n",
    "        coverage_metrics_by_gender[gender] = len(rec_set) / len(track_list)\n",
    "    \n",
    "    avg_recall_by_gender = {g: np.mean(scores) for g, scores in recall_by_gender.items()}\n",
    "    avg_diversity_by_gender = {g: np.mean(scores) for g, scores in diversity_by_gender.items()}\n",
    "\n",
    "    print(\"\\nEvaluation Metrics @ {}:\".format(top_n))\n",
    "    print(\"\\nOverall Recall: {:.4f}\".format(overall_recall))\n",
    "    print(\"Recall by gender:\", avg_recall_by_gender)\n",
    "\n",
    "    print(\"\\nOverall Coverage: {:.4f}\".format(overall_coverage))\n",
    "    print(\"Coverage by gender:\", coverage_metrics_by_gender)\n",
    "\n",
    "    print(\"\\nOverall Diversity: {:.4f}\".format(overall_diversity))\n",
    "    print(\"Diversity by gender:\", avg_diversity_by_gender)\n",
    "    \n",
    "    gender_metrics = {\n",
    "        'recall': avg_recall_by_gender,\n",
    "        'coverage': coverage_metrics_by_gender,\n",
    "        'diversity': avg_diversity_by_gender\n",
    "    }\n",
    "    \n",
    "    return overall_recall, overall_coverage, overall_diversity, gender_metrics\n",
    "\n",
    "def recGap_CF_results(df, gender_metrics):\n",
    "    for key, value in gender_metrics.items():\n",
    "        print(f\"\\nFor the {key} metric\")\n",
    "        compute_recGap(value)\n",
    "        compute_compounding_factor(df, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48631884",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5b446b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate_KNN(knn_model, user_track_matrix, sparse_item_matrix, track_list, df, df_val_holdout, df_test_holdout):\n",
    "    # Define candidate hyperparameters.\n",
    "    candidate_neighbors = [5, 10, 15]\n",
    "    candidate_top_n = [10]\n",
    "    best_params, best_ndcg, grid_results = grid_search_validation(knn_model, user_track_matrix, sparse_item_matrix, track_list, df, df_val_holdout, candidate_neighbors, candidate_top_n)\n",
    "    best_n_neighbors, best_top_n = best_params\n",
    "    overall_ndcg_test, ndcg_by_gender_test = evaluate_ndcg(knn_model, df, df_test_holdout, user_track_matrix, sparse_item_matrix, track_list, top_n=best_top_n, n_neighbors=best_n_neighbors)\n",
    "    overall_recall, overall_coverage, overall_diversity, gender_metrics = evaluate_metrics(knn_model, df, df_test_holdout, user_track_matrix, sparse_item_matrix, track_list, top_n=10, n_neighbors=10)\n",
    "    gender_metrics['ndcg'] = ndcg_by_gender_test \n",
    "    print(gender_metrics)\n",
    "    recGap_CF_results(df, gender_metrics)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65878b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_evaluate_knn(df):\n",
    "    df_model_train, df_val_holdout, df_test_holdout = build_data(df)\n",
    "    user_track_matrix, sparse_item_matrix, track_list = create_user_track_matrix(df_model_train)\n",
    "\n",
    "    # Train a KNN model on the item (track) vectors.\n",
    "    knn_model = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "    knn_model.fit(sparse_item_matrix)\n",
    "\n",
    "    Evaluate_KNN(knn_model, user_track_matrix, sparse_item_matrix, track_list, df, df_val_holdout, df_test_holdout)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90334e1d",
   "metadata": {},
   "source": [
    "# Running The Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab74aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv('data/LFM-1b-DemoBiasSub-10k.csv', header=0)\n",
    "df_SMOTE = pd.read_csv('data/LFM-1b-DemoBiasSub-10k-SMOTE.csv', header=0)\n",
    "df_resampled = pd.read_csv('data/LFM-1b-DemoBiasSub-10k-Resampled.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_and_evaluate_knn(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc617bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_and_evaluate_knn(df_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b2039",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_and_evaluate_knn(df_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74f7471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
