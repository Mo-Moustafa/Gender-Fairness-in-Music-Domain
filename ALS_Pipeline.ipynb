{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593b7392",
   "metadata": {},
   "source": [
    "# Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6904b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from Data_Splitter import build_data, create_user_track_matrix\n",
    "from Fairness_Metrics import compute_recGap, compute_compounding_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d32806",
   "metadata": {},
   "source": [
    "# ALS Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d047dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations_for_user_als(model, user_id, user_indices, item_indices, \n",
    "                                   user_track_matrix, track_list, top_n=10):\n",
    "    \"\"\"\n",
    "    Generate recommendations for a user using the trained ALS model.\n",
    "    \n",
    "    Parameters:\n",
    "        model: Trained ALS model\n",
    "        user_id: ID of the user to generate recommendations for\n",
    "        user_indices: Mapping from user IDs to matrix indices\n",
    "        item_indices: Mapping from track IDs to matrix indices\n",
    "        user_track_matrix: User-item interaction matrix\n",
    "        track_list: List of all track IDs\n",
    "        top_n: Number of recommendations to generate\n",
    "        \n",
    "    Returns:\n",
    "        List of recommended track IDs\n",
    "    \"\"\"\n",
    "    if user_id not in user_indices:\n",
    "        return []\n",
    "    \n",
    "    # Get the user's index in the model\n",
    "    user_idx = user_indices[user_id]\n",
    "    \n",
    "    # Get the user's interaction history to exclude already interacted items\n",
    "    if user_id in user_track_matrix.index:\n",
    "        user_history = set(user_track_matrix.loc[user_id][lambda row: row == 1].index)\n",
    "    else:\n",
    "        user_history = set()\n",
    "    \n",
    "    # Get recommendations from the model\n",
    "    # The recommend function returns a list of (item_id, score) tuples\n",
    "    recommendations = model.recommend(\n",
    "        userid=user_idx, \n",
    "        user_items=csr_matrix(np.zeros((1, len(item_indices)))), \n",
    "        N=top_n+len(user_history), \n",
    "        filter_already_liked_items=False\n",
    "    )\n",
    "    \n",
    "    # Convert item indices back to track IDs and filter out items in user history\n",
    "    rec_track_ids = []\n",
    "    for item_idx, _ in recommendations:  # Unpack properly as (item_idx, score)\n",
    "        # Get the track_id from item_indices by finding the key for the given value\n",
    "        reverse_item_indices = {v: k for k, v in item_indices.items()}\n",
    "        if item_idx in reverse_item_indices:\n",
    "            track_id = reverse_item_indices[item_idx]\n",
    "            if track_id not in user_history:\n",
    "                rec_track_ids.append(track_id)\n",
    "                if len(rec_track_ids) >= top_n:\n",
    "                    break\n",
    "                \n",
    "    return rec_track_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8495b309",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f209ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diversity_for_list(recommended_tracks, sparse_item_matrix, track_list):\n",
    "    \"\"\"\n",
    "    Compute intra-list diversity: average dissimilarity among all pairs of recommended tracks.\n",
    "    Dissimilarity is defined as (1 - cosine similarity) for each pair.\n",
    "    \"\"\"\n",
    "    if len(recommended_tracks) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Retrieve indices for the recommended tracks from track_list.\n",
    "    indices = [track_list.index(t) for t in recommended_tracks if t in track_list]\n",
    "    \n",
    "    if len(indices) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Extract the corresponding item vectors from the sparse matrix.\n",
    "    vectors = sparse_item_matrix[indices]\n",
    "    \n",
    "    # Compute pairwise cosine similarity.\n",
    "    sim_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    # Compute average pairwise similarity (ignoring the diagonal)\n",
    "    sum_similarity = 0.0\n",
    "    count = 0\n",
    "    n = len(indices)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            sum_similarity += sim_matrix[i, j]\n",
    "            count += 1\n",
    "    \n",
    "    avg_similarity = sum_similarity / count if count > 0 else 0.0\n",
    "    # Diversity is defined as the complement of similarity.\n",
    "    return 1 - avg_similarity\n",
    "\n",
    "def ndcg_at_k(relevances, k):\n",
    "    \"\"\"\n",
    "    Compute NDCG@k given a list of binary relevance scores.\n",
    "    \"\"\"\n",
    "    relevances = np.asfarray(relevances)[:k]\n",
    "    if relevances.size == 0:\n",
    "        return 0.0\n",
    "    # Discount factors (log2-based)\n",
    "    discounts = np.log2(np.arange(2, relevances.size + 2))\n",
    "    dcg = np.sum(relevances / discounts)\n",
    "    # Ideal DCG (sorted relevances)\n",
    "    ideal_relevances = np.sort(relevances)[::-1]\n",
    "    idcg = np.sum(ideal_relevances / discounts)\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def evaluate_als_metrics(als_model, df, holdout_df, user_track_matrix, sparse_item_matrix, \n",
    "                        track_list, user_indices, item_indices, top_n=10):\n",
    "    \"\"\"\n",
    "    Evaluate recommendations for all users in the holdout set using Recall@10, Coverage@10, and Diversity@10.\n",
    "    Also computes metrics for each gender subgroup.\n",
    "    \"\"\"\n",
    "    # Mapping from user_id to their ground truth track_ids.\n",
    "    user_holdout = holdout_df.groupby('user_id')['track_id'].apply(set).to_dict()\n",
    "    # Mapping from user_id to gender.\n",
    "    user_gender = df.set_index('user_id')['gender'].to_dict()\n",
    "    \n",
    "    recall_scores = {}\n",
    "    diversity_scores = {}\n",
    "    ndcg_scores = {}\n",
    "    # For coverage per gender, maintain a set of recommended items per gender.\n",
    "    coverage_by_gender = {}\n",
    "    \n",
    "    for user, ground_truth in user_holdout.items():\n",
    "        if user not in user_indices:\n",
    "            continue\n",
    "            \n",
    "        recs = get_recommendations_for_user_als(als_model, user, user_indices, item_indices, \n",
    "                                                user_track_matrix, track_list, top_n=top_n)\n",
    "        \n",
    "        # Skip if no recommendations were generated\n",
    "        if not recs:\n",
    "            continue\n",
    "        \n",
    "        # Compute Recall@10.\n",
    "        if ground_truth:\n",
    "            recall = len(set(recs).intersection(ground_truth)) / len(ground_truth)\n",
    "        else:\n",
    "            recall = 0.0\n",
    "        recall_scores[user] = recall\n",
    "        \n",
    "        # Compute NDCG@10\n",
    "        relevances = [1 if rec in ground_truth else 0 for rec in recs]\n",
    "        ndcg = ndcg_at_k(relevances, top_n)\n",
    "        ndcg_scores[user] = ndcg\n",
    "        \n",
    "        # Compute Diversity@10.\n",
    "        diversity = compute_diversity_for_list(recs, sparse_item_matrix, track_list)\n",
    "        diversity_scores[user] = diversity\n",
    "        \n",
    "        # Collect recommended items per gender for Coverage.\n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        if gender not in coverage_by_gender:\n",
    "            coverage_by_gender[gender] = set()\n",
    "        coverage_by_gender[gender].update(recs)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_recall = np.mean(list(recall_scores.values())) if recall_scores else 0.0\n",
    "    overall_diversity = np.mean(list(diversity_scores.values())) if diversity_scores else 0.0\n",
    "    overall_ndcg = np.mean(list(ndcg_scores.values())) if ndcg_scores else 0.0\n",
    "    \n",
    "    # Calculate coverage\n",
    "    all_recommended_items = set().union(*(coverage_by_gender.values())) if coverage_by_gender else set()\n",
    "    overall_coverage = len(all_recommended_items) / len(track_list) if track_list else 0.0\n",
    "    \n",
    "    # Compute per-gender averages.\n",
    "    recall_by_gender = {}\n",
    "    diversity_by_gender = {}\n",
    "    ndcg_by_gender = {}\n",
    "    coverage_metrics_by_gender = {}\n",
    "    \n",
    "    # Organize per-user metrics by gender.\n",
    "    for user, rec in recall_scores.items():\n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        if gender not in recall_by_gender:\n",
    "            recall_by_gender[gender] = []\n",
    "        recall_by_gender[gender].append(rec)\n",
    "    \n",
    "    for user, div in diversity_scores.items():\n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        if gender not in diversity_by_gender:\n",
    "            diversity_by_gender[gender] = []\n",
    "        diversity_by_gender[gender].append(div)\n",
    "    \n",
    "    for user, ndcg in ndcg_scores.items():\n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        if gender not in ndcg_by_gender:\n",
    "            ndcg_by_gender[gender] = []\n",
    "        ndcg_by_gender[gender].append(ndcg)\n",
    "    \n",
    "    for gender, rec_set in coverage_by_gender.items():\n",
    "        coverage_metrics_by_gender[gender] = len(rec_set) / len(track_list)\n",
    "    \n",
    "    # Calculate averages by gender\n",
    "    avg_recall_by_gender = {g: np.mean(scores) for g, scores in recall_by_gender.items()}\n",
    "    avg_diversity_by_gender = {g: np.mean(scores) for g, scores in diversity_by_gender.items()}\n",
    "    avg_ndcg_by_gender = {g: np.mean(scores) for g, scores in ndcg_by_gender.items()}\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"\\nEvaluation Metrics @ {}:\".format(top_n))\n",
    "    print(\"\\nOverall Recall: {:.4f}\".format(overall_recall))\n",
    "    print(\"Recall by gender:\", avg_recall_by_gender)\n",
    "\n",
    "    print(\"\\nOverall Coverage: {:.4f}\".format(overall_coverage))\n",
    "    print(\"Coverage by gender:\", coverage_metrics_by_gender)\n",
    "\n",
    "    print(\"\\nOverall Diversity: {:.4f}\".format(overall_diversity))\n",
    "    print(\"Diversity by gender:\", avg_diversity_by_gender)\n",
    "    \n",
    "    print(\"\\nOverall NDCG: {:.4f}\".format(overall_ndcg))\n",
    "    print(\"NDCG by gender:\", avg_ndcg_by_gender)\n",
    "    \n",
    "    gender_metrics = {\n",
    "        'recall': avg_recall_by_gender,\n",
    "        'coverage': coverage_metrics_by_gender,\n",
    "        'diversity': avg_diversity_by_gender,\n",
    "        'ndcg': avg_ndcg_by_gender\n",
    "    }\n",
    "    \n",
    "    return overall_recall, overall_coverage, overall_diversity, overall_ndcg, gender_metrics\n",
    "\n",
    "def grid_search_als(df, user_track_matrix, sparse_item_matrix, track_list, df_val_holdout, \n",
    "                    user_indices, item_indices, factors_list, regularization_list, iterations=15):\n",
    "    \"\"\"\n",
    "    Perform grid search over factors and regularization parameters for ALS model on validation set.\n",
    "    Returns the best hyperparameters (those that achieve the highest overall NDCG).\n",
    "    \"\"\"\n",
    "    best_ndcg = -1.0\n",
    "    best_params = None\n",
    "    grid_results = []  # Store tuples: (factors, regularization, overall_ndcg)\n",
    "    \n",
    "    # Convert sparse_item_matrix to CSR format for ALS\n",
    "    user_item_matrix_csr = csr_matrix((len(user_indices), len(item_indices)))\n",
    "    for user_id, user_idx in user_indices.items():\n",
    "        if user_id in user_track_matrix.index:\n",
    "            user_history = user_track_matrix.loc[user_id]\n",
    "            for track_id in user_history[user_history == 1].index:\n",
    "                if track_id in item_indices:\n",
    "                    item_idx = item_indices[track_id]\n",
    "                    user_item_matrix_csr[user_idx, item_idx] = 1\n",
    "    \n",
    "    for factors in factors_list:\n",
    "        for reg in regularization_list:\n",
    "            print(f\"\\nTraining ALS model with factors={factors}, regularization={reg}\")\n",
    "            # Train ALS model with current hyperparameters\n",
    "            als_model = AlternatingLeastSquares(factors=factors, regularization=reg, \n",
    "                                                iterations=iterations, random_state=42)\n",
    "            \n",
    "            try:\n",
    "                als_model.fit(user_item_matrix_csr)\n",
    "                \n",
    "                # Evaluate on validation set\n",
    "                _, _, _, overall_ndcg, _ = evaluate_als_metrics(\n",
    "                    als_model, df, df_val_holdout, user_track_matrix, \n",
    "                    sparse_item_matrix, track_list, user_indices, item_indices\n",
    "                )\n",
    "                \n",
    "                grid_results.append((factors, reg, overall_ndcg))\n",
    "                print(f\"Factors: {factors}, Regularization: {reg} => NDCG: {overall_ndcg:.4f}\")\n",
    "                \n",
    "                if overall_ndcg > best_ndcg:\n",
    "                    best_ndcg = overall_ndcg\n",
    "                    best_params = (factors, reg)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with factors={factors}, reg={reg}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if best_params is None:\n",
    "        print(\"No successful parameter combination found. Using default values.\")\n",
    "        best_params = (factors_list[0], regularization_list[0])\n",
    "    else:\n",
    "        print(\"\\nBest hyperparameters (factors, regularization):\", best_params)\n",
    "        print(\"Best overall NDCG on validation set:\", best_ndcg)\n",
    "    \n",
    "    return best_params, best_ndcg, grid_results\n",
    "\n",
    "def recGap_ALS_results(df, gender_metrics):\n",
    "    for key, value in gender_metrics.items():\n",
    "        print(f\"\\nFor the {key} metric\")\n",
    "        compute_recGap(value)\n",
    "        compute_compounding_factor(df, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8bd4b1",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b87b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_evaluate_als(df):\n",
    "    # Split data into train, validation, and test sets\n",
    "    print(\"Building data splits...\")\n",
    "    df_model_train, df_val_holdout, df_test_holdout = build_data(df)\n",
    "    \n",
    "    # Create user-track matrix and get sparse item matrix\n",
    "    print(\"Creating user-track matrix...\")\n",
    "    user_track_matrix, sparse_item_matrix, track_list = create_user_track_matrix(df_model_train)\n",
    "    \n",
    "    # Create user and item indices for ALS\n",
    "    print(\"Creating user and item indices...\")\n",
    "    unique_users = df_model_train['user_id'].unique()\n",
    "    unique_tracks = track_list\n",
    "    \n",
    "    user_indices = {user_id: idx for idx, user_id in enumerate(unique_users)}\n",
    "    item_indices = {track_id: idx for idx, track_id in enumerate(unique_tracks)}\n",
    "    \n",
    "    # Convert to CSR matrix for ALS\n",
    "    print(\"Converting to CSR matrix...\")\n",
    "    user_item_matrix_csr = csr_matrix((len(user_indices), len(item_indices)))\n",
    "    for user_id, user_idx in user_indices.items():\n",
    "        if user_id in user_track_matrix.index:\n",
    "            user_history = user_track_matrix.loc[user_id]\n",
    "            for track_id in user_history[user_history == 1].index:\n",
    "                if track_id in item_indices:\n",
    "                    item_idx = item_indices[track_id]\n",
    "                    user_item_matrix_csr[user_idx, item_idx] = 1\n",
    "    \n",
    "    print(f\"Matrix shape: {user_item_matrix_csr.shape}\")\n",
    "    print(f\"Non-zero entries: {user_item_matrix_csr.nnz}\")\n",
    "    \n",
    "    # Define hyperparameter search space\n",
    "    factors_list = [50, 100]  # Reduced for quicker execution\n",
    "    regularization_list = [0.01, 0.1]  # Reduced for quicker execution\n",
    "    \n",
    "    # Perform grid search on validation set\n",
    "    # print(\"\\nStarting grid search...\")\n",
    "    # best_params, _, _ = grid_search_als(\n",
    "    #     df, user_track_matrix, sparse_item_matrix, track_list, \n",
    "    #     df_val_holdout, user_indices, item_indices, \n",
    "    #     factors_list, regularization_list\n",
    "    # )\n",
    "    \n",
    "    best_factors, best_reg = 50, 0.01\n",
    "    \n",
    "    # Train final ALS model with best hyperparameters\n",
    "    print(f\"\\nTraining final ALS model with factors={best_factors}, regularization={best_reg}\")\n",
    "    final_als_model = AlternatingLeastSquares(\n",
    "        factors=best_factors, \n",
    "        regularization=best_reg,\n",
    "        iterations=15,\n",
    "        random_state=42\n",
    "    )\n",
    "    final_als_model.fit(user_item_matrix_csr)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    overall_recall, overall_coverage, overall_diversity, overall_ndcg, gender_metrics = evaluate_als_metrics(\n",
    "        final_als_model, df, df_test_holdout, user_track_matrix, \n",
    "        sparse_item_matrix, track_list, user_indices, item_indices, top_n=10\n",
    "    )\n",
    "    \n",
    "    # Calculate fairness metrics\n",
    "    print(\"\\nFairness Metrics:\")\n",
    "    recGap_ALS_results(df, gender_metrics)\n",
    "    \n",
    "    return final_als_model, gender_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3543b4",
   "metadata": {},
   "source": [
    "# Running the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae50566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and run the ALS pipeline.\n",
    "df = pd.read_csv('data/LFM-1b-DemoBiasSub-10k.csv', header=0)\n",
    "df_SMOTE = pd.read_csv('data/LFM-1b-DemoBiasSub-10k-SMOTE.csv', header=0)\n",
    "df_resampled = pd.read_csv('data/LFM-1b-DemoBiasSub-10k-Resampled.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a128b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building data splits...\n",
      "Creating user-track matrix...\n",
      "Creating user and item indices...\n",
      "Converting to CSR matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20115\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_index.py:108: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (756, 3961)\n",
      "Non-zero entries: 9088\n",
      "\n",
      "Training final ALS model with factors=50, regularization=0.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ce824b659a4748af5121626f2bed47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbuild_and_evaluate_als\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 58\u001b[0m, in \u001b[0;36mbuild_and_evaluate_als\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m overall_recall, overall_coverage, overall_diversity, overall_ndcg, gender_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_als_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_als_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test_holdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_track_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_item_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m     61\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Calculate fairness metrics\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFairness Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 70\u001b[0m, in \u001b[0;36mevaluate_als_metrics\u001b[1;34m(als_model, df, holdout_df, user_track_matrix, sparse_item_matrix, track_list, user_indices, item_indices, top_n)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m user_indices:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m recs \u001b[38;5;241m=\u001b[39m \u001b[43mget_recommendations_for_user_als\u001b[49m\u001b[43m(\u001b[49m\u001b[43mals_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43muser_track_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Skip if no recommendations were generated\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m recs:\n",
      "Cell \u001b[1;32mIn[12], line 41\u001b[0m, in \u001b[0;36mget_recommendations_for_user_als\u001b[1;34m(model, user_id, user_indices, item_indices, user_track_matrix, track_list, top_n)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Convert item indices back to track IDs and filter out items in user history\u001b[39;00m\n\u001b[0;32m     40\u001b[0m rec_track_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item_idx, _ \u001b[38;5;129;01min\u001b[39;00m recommendations:  \u001b[38;5;66;03m# Unpack properly as (item_idx, score)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Get the track_id from item_indices by finding the key for the given value\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     reverse_item_indices \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m item_indices\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item_idx \u001b[38;5;129;01min\u001b[39;00m reverse_item_indices:\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "build_and_evaluate_als(df[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d2de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_and_evaluate_als(df_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8951af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_and_evaluate_als(df_resampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
