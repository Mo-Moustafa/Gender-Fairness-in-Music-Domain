{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd2e9ad7",
   "metadata": {},
   "source": [
    "# Neccessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3878c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from Data_Splitter import build_data, create_user_track_matrix\n",
    "from Fairness_Metrics import compute_recGap, compute_compounding_factor\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce50cbdf",
   "metadata": {},
   "source": [
    "# KNNItem Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c24636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function finds the most similar tracks to a given track, using a k-nearest neighbors (KNN) model and a sparse matrix representing track features.\n",
    "def get_item_similarities(knn_model, track_id, sparse_item_matrix, track_list, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Retrieve the n most similar tracks for a given track_id.\n",
    "    \"\"\"\n",
    "    if track_id not in track_list:\n",
    "        return []\n",
    "    track_index = track_list.index(track_id)\n",
    "    track_vector = sparse_item_matrix[track_index]\n",
    "    # Retrieve neighbors (n_neighbors + 1 because the track itself is returned)\n",
    "    distances, indices = knn_model.kneighbors(track_vector, n_neighbors=n_neighbors + 1)\n",
    "    # Skip the first index if it is the track itself.\n",
    "    similar_indices = [i for i in indices.flatten() if i != track_index]\n",
    "    similar_tracks = [track_list[i] for i in similar_indices]\n",
    "    return similar_tracks\n",
    "\n",
    "\n",
    "#To generate track recommendations for a specific user, by looking at the tracks they’ve already interacted with and suggesting similar tracks they haven’t heard yet.\n",
    "def get_recommendations_for_user(knn_model, user_id, user_track_matrix, sparse_item_matrix, track_list, top_n=10, n_neighbors=10):\n",
    "    \"\"\"\n",
    "    For a given user, aggregate similar items from the items the user has interacted with in the training data.\n",
    "    Only recommend items the user has not interacted with.\n",
    "    \"\"\"\n",
    "    if user_id not in user_track_matrix.index:\n",
    "        return []\n",
    "    # Get the set of tracks the user has interacted with (training interactions)\n",
    "    user_history = set(user_track_matrix.loc[user_id][lambda row: row == 1].index)\n",
    "\n",
    "    candidate_scores = {}\n",
    "    # For each item in the user history, get similar items and sum a simple frequency score.\n",
    "    for item in user_history:\n",
    "        similar_items = get_item_similarities(knn_model, item, sparse_item_matrix, track_list, n_neighbors=n_neighbors)\n",
    "        for sim_item in similar_items:\n",
    "            if sim_item in user_history:\n",
    "                continue\n",
    "            candidate_scores[sim_item] = candidate_scores.get(sim_item, 0) + 1\n",
    "\n",
    "    ranked_items = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    recommended_items = [item for item, score in ranked_items][:top_n]\n",
    "    return recommended_items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992f85e",
   "metadata": {},
   "source": [
    "## KNNItem Evaluation Using NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calculates NDCG@k — a normalized score evaluating how good the top-k recommendations are.\n",
    "def ndcg_at_k(relevances, k):\n",
    "    \"\"\"\n",
    "    Compute NDCG@k given a list of binary relevance scores.\n",
    "    \"\"\"\n",
    "    relevances = np.asfarray(relevances)[:k]\n",
    "    if relevances.size == 0:\n",
    "        return 0.0\n",
    "    # Discount factors (log2-based)\n",
    "    discounts = np.log2(np.arange(2, relevances.size + 2))\n",
    "    dcg = np.sum(relevances / discounts)\n",
    "    # Ideal DCG (sorted relevances)\n",
    "    ideal_relevances = np.sort(relevances)[::-1]\n",
    "    idcg = np.sum(ideal_relevances / discounts)\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "#For each user in a holdout test set, generate top-n recommendations and compute NDCG@k, which measures how well the recommended items match the user's actual held-back (ground truth) interactions. Also, analyze performance across different gender groups.\n",
    "def evaluate_ndcg(knn_model, df, holdout_df, user_track_matrix, sparse_item_matrix, track_list, top_n=10, n_neighbors=10):\n",
    "    \"\"\"\n",
    "    Evaluate recommendations using NDCG@k for each user in a holdout set.\n",
    "    Returns overall NDCG and NDCG by gender.\n",
    "    \"\"\"\n",
    "    # Create mapping from user_id to their holdout (ground truth) track_ids.\n",
    "    user_holdout = holdout_df.groupby('user_id')['track_id'].apply(set).to_dict()\n",
    "    # Get user genders from the original data (assuming 'gender' column exists).\n",
    "    user_gender = df.set_index('user_id')['gender'].to_dict()\n",
    "    \n",
    "    ndcg_scores = {}    # per user scores\n",
    "    ndcg_by_gender = {} # aggregated scores per gender\n",
    "    \n",
    "    for user, true_items in user_holdout.items():\n",
    "        # Generate recommendations using the training data.\n",
    "        recs = get_recommendations_for_user(knn_model, user, user_track_matrix, sparse_item_matrix, track_list, top_n=top_n, n_neighbors=n_neighbors)\n",
    "        # Binary relevance: 1 if the recommended item is in the holdout set, 0 otherwise.\n",
    "        relevances = [1 if rec in true_items else 0 for rec in recs]\n",
    "        ndcg = ndcg_at_k(relevances, top_n)\n",
    "        ndcg_scores[user] = ndcg\n",
    "        \n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        if gender not in ndcg_by_gender:\n",
    "            ndcg_by_gender[gender] = []\n",
    "        ndcg_by_gender[gender].append(ndcg)\n",
    "    \n",
    "    overall_ndcg = np.mean(list(ndcg_scores.values())) if ndcg_scores else 0.0\n",
    "    avg_ndcg_by_gender = {gender: np.mean(scores) for gender, scores in ndcg_by_gender.items()}\n",
    "\n",
    "    print(\"\\nSet Evaluation:\")\n",
    "    print(f\"Overall NDCG@{top_n}: {overall_ndcg:.4f}\")\n",
    "    print(\"NDCG by gender:\", avg_ndcg_by_gender)\n",
    "\n",
    "    return overall_ndcg, avg_ndcg_by_gender\n",
    "\n",
    "#find the best configuration for your recommendation system.\n",
    "def grid_search_validation(knn_model, user_track_matrix, sparse_item_matrix, track_list, df, val_holdout_df, candidate_neighbors, candidate_top_n):\n",
    "    \"\"\"\n",
    "    Perform grid search over n_neighbors and top_n parameters on the validation holdout set.\n",
    "    Returns the best hyperparameters (those that achieve the highest overall NDCG) and grid search results.\n",
    "    \"\"\"\n",
    "    # Initialize best NDCG and parameter holders\n",
    "    best_ndcg = -1.0\n",
    "    best_params = None\n",
    "    grid_results = []  # Store tuples: (n_neighbors, top_n, overall_ndcg)\n",
    "   \n",
    "    # Try every combination of candidate parameters\n",
    "    for n_neighbors_param in candidate_neighbors:\n",
    "        for top_n_param in candidate_top_n:\n",
    "            # Evaluate NDCG using the current parameter combo\n",
    "            overall_ndcg_val, _ = evaluate_ndcg(knn_model, df, val_holdout_df, user_track_matrix, sparse_item_matrix, track_list, top_n=top_n_param, n_neighbors=n_neighbors_param)\n",
    "           # Save result for reporting\n",
    "            grid_results.append((n_neighbors_param, top_n_param, overall_ndcg_val))\n",
    "            print(f\"n_neighbors: {n_neighbors_param}, top_n: {top_n_param} => NDCG: {overall_ndcg_val:.4f}\")\n",
    "            \n",
    "            # Update best parameters if this NDCG is the highest so far\n",
    "            if overall_ndcg_val > best_ndcg:\n",
    "                best_ndcg = overall_ndcg_val\n",
    "                best_params = (n_neighbors_param, top_n_param)\n",
    "\n",
    "    print(\"\\nBest hyperparameters (n_neighbors, top_n):\", best_params)\n",
    "    print(\"Best overall NDCG on validation set:\", best_ndcg)\n",
    "    \n",
    "    return best_params, best_ndcg, grid_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b48f70a",
   "metadata": {},
   "source": [
    "# Other Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06336e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diversity_for_list(recommended_tracks, sparse_item_matrix, track_list):\n",
    "    \"\"\"\n",
    "    Compute intra-list diversity: average dissimilarity among all pairs of recommended tracks.\n",
    "    Dissimilarity is defined as (1 - cosine similarity) for each pair.\n",
    "    \"\"\"\n",
    "    if len(recommended_tracks) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Retrieve indices for the recommended tracks from track_list.\n",
    "    indices = [track_list.index(t) for t in recommended_tracks if t in track_list]\n",
    "    \n",
    "    # Extract the corresponding item vectors from the sparse matrix.\n",
    "    vectors = sparse_item_matrix[indices]\n",
    "    \n",
    "    # Compute pairwise cosine similarity.\n",
    "    sim_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    # Compute average pairwise similarity (ignoring the diagonal)\n",
    "    sum_similarity = 0.0\n",
    "    count = 0\n",
    "    n = len(indices)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            sum_similarity += sim_matrix[i, j]\n",
    "            count += 1\n",
    "    \n",
    "    avg_similarity = sum_similarity / count if count > 0 else 0.0\n",
    "    # Diversity is defined as the complement of similarity.\n",
    "    return 1 - avg_similarity\n",
    "\n",
    "def evaluate_metrics(knn_model, df, holdout_df, user_track_matrix, sparse_item_matrix, track_list, top_n=10, n_neighbors=10):\n",
    "    \"\"\"\n",
    "    Evaluate recommendations for all users in the holdout set using Recall@10, Coverage@10, and Diversity@10.\n",
    "    Also, compute the metrics for each gender subgroup.\n",
    "    \n",
    "    Parameters:\n",
    "        knn_model         : The trained KNN model.\n",
    "        holdout_df        : DataFrame with ground-truth interactions (must include 'user_id' and 'track_id').\n",
    "        user_track_matrix : Training user-by-item matrix.\n",
    "        sparse_item_matrix: Sparse representation of item vectors.\n",
    "        track_list        : List of track IDs.\n",
    "        df                : The original DataFrame containing user attributes (e.g., 'gender').\n",
    "        top_n             : Number of recommendations per user.\n",
    "        n_neighbors       : Number of neighbors to consider.\n",
    "        \n",
    "        Returns:\n",
    "        overall_recall, overall_coverage, overall_diversity, and a dictionary `gender_metrics`\n",
    "        that contains per-gender averages for recall, coverage, and diversity.\n",
    "    \"\"\"\n",
    "    # Mapping from user_id to their ground truth track_ids.\n",
    "    user_holdout = holdout_df.groupby('user_id')['track_id'].apply(set).to_dict()\n",
    "    # Mapping from user_id to gender.\n",
    "    user_gender = df.set_index('user_id')['gender'].to_dict()\n",
    "    \n",
    "    recall_scores = {}\n",
    "    diversity_scores = {}\n",
    "    # For coverage per gender, maintain a set of recommended items per gender.\n",
    "    coverage_by_gender = {}\n",
    "    \n",
    "    for user, ground_truth in user_holdout.items():\n",
    "        recs = get_recommendations_for_user(knn_model, user, user_track_matrix, sparse_item_matrix, track_list, top_n=top_n, n_neighbors=n_neighbors)\n",
    "        \n",
    "        # Compute Recall@10.\n",
    "        if ground_truth:\n",
    "            recall = len(set(recs).intersection(ground_truth)) / len(ground_truth)\n",
    "        else:\n",
    "            recall = 0.0\n",
    "        recall_scores[user] = recall\n",
    "        \n",
    "        # Compute Diversity@10.\n",
    "        diversity = compute_diversity_for_list(recs, sparse_item_matrix, track_list)\n",
    "        diversity_scores[user] = diversity\n",
    "        \n",
    "        # Collect recommended items per gender for Coverage.\n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        if gender not in coverage_by_gender:\n",
    "            coverage_by_gender[gender] = set()\n",
    "        coverage_by_gender[gender].update(recs)\n",
    "    \n",
    "    overall_recall = np.mean(list(recall_scores.values()))\n",
    "    overall_diversity = np.mean(list(diversity_scores.values()))\n",
    "    overall_coverage = len(set().union(*(set(recs) for recs in coverage_by_gender.values()))) / len(track_list)\n",
    "    \n",
    "    # Compute per-gender averages.\n",
    "    recall_by_gender = {}\n",
    "    diversity_by_gender = {}\n",
    "    coverage_metrics_by_gender = {}\n",
    "    \n",
    "    # Organize per-user metrics by gender.\n",
    "    for user, rec in recall_scores.items():\n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        if gender not in recall_by_gender:\n",
    "            recall_by_gender[gender] = []\n",
    "        recall_by_gender[gender].append(rec)\n",
    "    \n",
    "    for user, div in diversity_scores.items():\n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        if gender not in diversity_by_gender:\n",
    "            diversity_by_gender[gender] = []\n",
    "        diversity_by_gender[gender].append(div)\n",
    "    \n",
    "    for gender, rec_set in coverage_by_gender.items():\n",
    "        coverage_metrics_by_gender[gender] = len(rec_set) / len(track_list)\n",
    "    \n",
    "    avg_recall_by_gender = {g: np.mean(scores) for g, scores in recall_by_gender.items()}\n",
    "    avg_diversity_by_gender = {g: np.mean(scores) for g, scores in diversity_by_gender.items()}\n",
    "\n",
    "    print(\"\\nEvaluation Metrics @ {}:\".format(top_n))\n",
    "    print(\"\\nOverall Recall: {:.4f}\".format(overall_recall))\n",
    "    print(\"Recall by gender:\", avg_recall_by_gender)\n",
    "\n",
    "    print(\"\\nOverall Coverage: {:.4f}\".format(overall_coverage))\n",
    "    print(\"Coverage by gender:\", coverage_metrics_by_gender)\n",
    "\n",
    "    print(\"\\nOverall Diversity: {:.4f}\".format(overall_diversity))\n",
    "    print(\"Diversity by gender:\", avg_diversity_by_gender)\n",
    "    \n",
    "    gender_metrics = {\n",
    "        'recall': avg_recall_by_gender,\n",
    "        'coverage': coverage_metrics_by_gender,\n",
    "        'diversity': avg_diversity_by_gender\n",
    "    }\n",
    "    \n",
    "    return overall_recall, overall_coverage, overall_diversity, gender_metrics\n",
    "\n",
    "def recGap_CF_results(df, gender_metrics):\n",
    "    for key, value in gender_metrics.items():\n",
    "        print(f\"\\nFor the {key} metric\")\n",
    "        compute_recGap(value)\n",
    "        compute_compounding_factor(df, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48631884",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b446b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate a K-Nearest Neighbors (KNN) recommendation model across multiple metrics and using different sets of data\n",
    "def Evaluate_KNN(knn_model, user_track_matrix, sparse_item_matrix, track_list, df, df_val_holdout, df_test_holdout):\n",
    "    # Define candidate hyperparameters.\n",
    "    candidate_neighbors = [5, 10, 15]\n",
    "    candidate_top_n = [10]\n",
    "    best_params, best_ndcg, grid_results = grid_search_validation(knn_model, user_track_matrix, sparse_item_matrix, track_list, df, df_val_holdout, candidate_neighbors, candidate_top_n)\n",
    "    best_n_neighbors, best_top_n = best_params\n",
    "    overall_ndcg_test, ndcg_by_gender_test = evaluate_ndcg(knn_model, df, df_test_holdout, user_track_matrix, sparse_item_matrix, track_list, top_n=best_top_n, n_neighbors=best_n_neighbors)\n",
    "    overall_recall, overall_coverage, overall_diversity, gender_metrics = evaluate_metrics(knn_model, df, df_test_holdout, user_track_matrix, sparse_item_matrix, track_list, top_n=10, n_neighbors=10)\n",
    "    gender_metrics['ndcg'] = ndcg_by_gender_test \n",
    "    print(gender_metrics)\n",
    "    recGap_CF_results(df, gender_metrics)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65878b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unction is responsible for building, training, and evaluating a K-Nearest Neighbors (KNN) recommender system for a given dataset (df)    \n",
    "def build_and_evaluate_knn(df):\n",
    "    df_model_train, df_val_holdout, df_test_holdout = build_data(df)\n",
    "    user_track_matrix, sparse_item_matrix, track_list = create_user_track_matrix(df_model_train)\n",
    "\n",
    "    # Train a KNN model on the item (track) vectors.\n",
    "    knn_model = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "    knn_model.fit(sparse_item_matrix)\n",
    "\n",
    "    Evaluate_KNN(knn_model, user_track_matrix, sparse_item_matrix, track_list, df, df_val_holdout, df_test_holdout)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90334e1d",
   "metadata": {},
   "source": [
    "# Running The Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab74aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv('data/LFM-1b-DemoBiasSub-10k.csv', header=0)\n",
    "df_SMOTE = pd.read_csv('data/LFM-1b-DemoBiasSub-10k-SMOTE.csv', header=0)\n",
    "df_resampled = pd.read_csv('data/LFM-1b-DemoBiasSub-10k-Resampled.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dc2c08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.1665\n",
      "NDCG by gender: {'m': 0.16873887963637296, 'f': 0.16029092281791665}\n",
      "n_neighbors: 5, top_n: 10 => NDCG: 0.1665\n",
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.1613\n",
      "NDCG by gender: {'m': 0.16309032354503913, 'f': 0.15645769099064857}\n",
      "n_neighbors: 10, top_n: 10 => NDCG: 0.1613\n",
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.1591\n",
      "NDCG by gender: {'m': 0.16149819846488775, 'f': 0.15255109670918537}\n",
      "n_neighbors: 15, top_n: 10 => NDCG: 0.1591\n",
      "\n",
      "Best hyperparameters (n_neighbors, top_n): (5, 10)\n",
      "Best overall NDCG on validation set: 0.1664797411405441\n",
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.1624\n",
      "NDCG by gender: {'m': 0.16615608516935818, 'f': 0.15200149805676685}\n",
      "\n",
      "Evaluation Metrics @ 10:\n",
      "\n",
      "Overall Recall: 0.1266\n",
      "Recall by gender: {'m': 0.12713856915277838, 'f': 0.12509383567976654}\n",
      "\n",
      "Overall Coverage: 0.8301\n",
      "Coverage by gender: {'m': 0.7821, 'f': 0.5344}\n",
      "\n",
      "Overall Diversity: 0.9358\n",
      "Diversity by gender: {'m': 0.9352221131847291, 'f': 0.9374286046984478}\n",
      "{'recall': {'m': 0.12713856915277838, 'f': 0.12509383567976654}, 'coverage': {'m': 0.7821, 'f': 0.5344}, 'diversity': {'m': 0.9352221131847291, 'f': 0.9374286046984478}, 'ndcg': {'m': 0.16615608516935818, 'f': 0.15200149805676685}}\n",
      "\n",
      "For the recall metric\n",
      "RecGap Score: 0.002044733473011845 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.7575144921328422\n",
      "Females: 0.24248550786715783\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.7604802323389963\n",
      "Females: 0.23951976766100375\n",
      "\n",
      "Compounding Factor (KL Divergence): 2.4076088160425186e-05\n",
      "\n",
      "For the coverage metric\n",
      "RecGap Score: 0.24770000000000003 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.7575144921328422\n",
      "Females: 0.24248550786715783\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.8205294549886283\n",
      "Females: 0.17947054501137175\n",
      "\n",
      "Compounding Factor (KL Divergence): 0.01244055280110478\n",
      "\n",
      "For the diversity metric\n",
      "RecGap Score: 0.002206491513718656 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.7575144921328422\n",
      "Females: 0.24248550786715783\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.7570813645530419\n",
      "Females: 0.24291863544695808\n",
      "\n",
      "Compounding Factor (KL Divergence): 5.102390991167604e-07\n",
      "\n",
      "For the ndcg metric\n",
      "RecGap Score: 0.014154587112591321 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.7575144921328422\n",
      "Females: 0.24248550786715783\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.7734925082231681\n",
      "Females: 0.2265074917768319\n",
      "\n",
      "Compounding Factor (KL Divergence): 0.0007169252234241787\n"
     ]
    }
   ],
   "source": [
    "build_and_evaluate_knn(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccc617bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.1311\n",
      "NDCG by gender: {'m': 0.17106645297937007, 'f': 0.03281492641683718}\n",
      "n_neighbors: 5, top_n: 10 => NDCG: 0.1311\n",
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.1241\n",
      "NDCG by gender: {'m': 0.16223132004611746, 'f': 0.030498970041845486}\n",
      "n_neighbors: 10, top_n: 10 => NDCG: 0.1241\n",
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.1256\n",
      "NDCG by gender: {'m': 0.16461395505395032, 'f': 0.02958040476094506}\n",
      "n_neighbors: 15, top_n: 10 => NDCG: 0.1256\n",
      "\n",
      "Best hyperparameters (n_neighbors, top_n): (5, 10)\n",
      "Best overall NDCG on validation set: 0.13109800217219233\n",
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.1284\n",
      "NDCG by gender: {'m': 0.166356275771398, 'f': 0.03002186182458126}\n",
      "\n",
      "Evaluation Metrics @ 10:\n",
      "\n",
      "Overall Recall: 0.0970\n",
      "Recall by gender: {'m': 0.13003709918633072, 'f': 0.011583962980876501}\n",
      "\n",
      "Overall Coverage: 0.8312\n",
      "Coverage by gender: {'m': 0.7819, 'f': 0.52}\n",
      "\n",
      "Overall Diversity: 0.9362\n",
      "Diversity by gender: {'m': 0.9357073085682485, 'f': 0.9375656741996905}\n",
      "{'recall': {'m': 0.13003709918633072, 'f': 0.011583962980876501}, 'coverage': {'m': 0.7819, 'f': 0.52}, 'diversity': {'m': 0.9357073085682485, 'f': 0.9375656741996905}, 'ndcg': {'m': 0.166356275771398, 'f': 0.03002186182458126}}\n",
      "\n",
      "For the recall metric\n",
      "RecGap Score: 0.11845313620545422 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.5\n",
      "Females: 0.5\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.9182045184267881\n",
      "Females: 0.08179548157321195\n",
      "\n",
      "Compounding Factor (KL Divergence): 0.6012870197440889\n",
      "\n",
      "For the coverage metric\n",
      "RecGap Score: 0.2619 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.5\n",
      "Females: 0.5\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.6005837621937169\n",
      "Females: 0.3994162378062831\n",
      "\n",
      "Compounding Factor (KL Divergence): 0.020655000990435476\n",
      "\n",
      "For the diversity metric\n",
      "RecGap Score: 0.0018583656314420383 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.5\n",
      "Females: 0.5\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.4995039789586096\n",
      "Females: 0.5004960210413903\n",
      "\n",
      "Compounding Factor (KL Divergence): 4.92073989198389e-07\n",
      "\n",
      "For the ndcg metric\n",
      "RecGap Score: 0.13633441394681672 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.5\n",
      "Females: 0.5\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.8471221787104067\n",
      "Females: 0.15287782128959318\n",
      "\n",
      "Compounding Factor (KL Divergence): 0.32886610755337026\n"
     ]
    }
   ],
   "source": [
    "build_and_evaluate_knn(df_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "528b2039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.1466\n",
      "NDCG by gender: {'m': 0.1422848312667832, 'f': 0.15107085047906227, nan: 0.2072790969450856}\n",
      "n_neighbors: 5, top_n: 10 => NDCG: 0.1466\n",
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.1412\n",
      "NDCG by gender: {'m': 0.13973483805043932, 'f': 0.1414955846832796, nan: 0.1839761793813193}\n",
      "n_neighbors: 10, top_n: 10 => NDCG: 0.1412\n",
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.1423\n",
      "NDCG by gender: {'m': 0.13941551067597457, 'f': 0.14583870070935803, nan: 0.17156387036702583}\n",
      "n_neighbors: 15, top_n: 10 => NDCG: 0.1423\n",
      "\n",
      "Best hyperparameters (n_neighbors, top_n): (5, 10)\n",
      "Best overall NDCG on validation set: 0.14658771638839907\n",
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.1441\n",
      "NDCG by gender: {'m': 0.13612504082336677, 'f': 0.15453662499907928, nan: 0.2530286660714235}\n",
      "\n",
      "Evaluation Metrics @ 10:\n",
      "\n",
      "Overall Recall: 0.1114\n",
      "Recall by gender: {'m': 0.10792699554269129, 'f': 0.11827631386824566, nan: 0.1048894557823129}\n",
      "\n",
      "Overall Coverage: 0.8308\n",
      "Coverage by gender: {'m': 0.7602, 'f': 0.6086, nan: 0.0649}\n",
      "\n",
      "Overall Diversity: 0.9353\n",
      "Diversity by gender: {'m': 0.9353886356838003, 'f': 0.9348953258094914, nan: 0.9406077430077507}\n",
      "{'recall': {'m': 0.10792699554269129, 'f': 0.11827631386824566, nan: 0.1048894557823129}, 'coverage': {'m': 0.7602, 'f': 0.6086, nan: 0.0649}, 'diversity': {'m': 0.9353886356838003, 'f': 0.9348953258094914, nan: 0.9406077430077507}, 'ndcg': {'m': 0.13612504082336677, 'f': 0.15453662499907928, nan: 0.2530286660714235}}\n",
      "\n",
      "For the recall metric\n",
      "RecGap Score: 0.010349318325554371 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.48437451498457995\n",
      "Females: 0.48437451498457995\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.477123857399555\n",
      "Females: 0.5228761426004451\n",
      "\n",
      "Compounding Factor (KL Divergence): -0.029742500612872355\n",
      "\n",
      "For the coverage metric\n",
      "RecGap Score: 0.15159999999999996 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.48437451498457995\n",
      "Females: 0.48437451498457995\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.5553769725306837\n",
      "Females: 0.4446230274693162\n",
      "\n",
      "Compounding Factor (KL Divergence): -0.024779199984091803\n",
      "\n",
      "For the diversity metric\n",
      "RecGap Score: 0.0004933098743088449 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.48437451498457995\n",
      "Females: 0.48437451498457995\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.5001318810096396\n",
      "Females: 0.49986811899036043\n",
      "\n",
      "Compounding Factor (KL Divergence): -0.030757457027256345\n",
      "\n",
      "For the ndcg metric\n",
      "RecGap Score: 0.01841158417571251 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.48437451498457995\n",
      "Females: 0.48437451498457995\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.4683281520395617\n",
      "Females: 0.5316718479604384\n",
      "\n",
      "Compounding Factor (KL Divergence): -0.02881006532108369\n"
     ]
    }
   ],
   "source": [
    "build_and_evaluate_knn(df_SMOTE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
