{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cadc9208",
      "metadata": {
        "id": "cadc9208"
      },
      "source": [
        "# Neccessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_ghkNCeqYZbj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ghkNCeqYZbj",
        "outputId": "7d75b097-c44a-4a00-d04e-8c8039fd0440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tqdm_joblib\n",
            "  Downloading tqdm_joblib-0.0.4-py3-none-any.whl.metadata (269 bytes)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tqdm_joblib) (4.67.1)\n",
            "Downloading tqdm_joblib-0.0.4-py3-none-any.whl (1.7 kB)\n",
            "Installing collected packages: tqdm_joblib\n",
            "Successfully installed tqdm_joblib-0.0.4\n"
          ]
        }
      ],
      "source": [
        "%pip install tqdm_joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e10d9e31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e10d9e31",
        "outputId": "fb86bad1-8316-407c-a8f4-a74fddc58346"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tqdm_joblib/__init__.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "from tqdm_joblib import tqdm_joblib\n",
        "from Data_Splitter import build_data, create_user_track_matrix\n",
        "from Fairness_Metrics import compute_recGap, compute_compounding_factor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "771cf053",
      "metadata": {
        "id": "771cf053"
      },
      "source": [
        "# SLIM Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae8e5662",
      "metadata": {
        "id": "ae8e5662"
      },
      "outputs": [],
      "source": [
        "def train_item(j, X, sample_weights, alpha, l1_ratio, max_iter, tol):\n",
        "    \"\"\"\n",
        "    Helper function to train ElasticNet for one item (column).\n",
        "    This function excludes item j from X and fits a model to predict column j.\n",
        "    Returns the weight vector of length n_items with zero inserted at index j.\n",
        "    \"\"\"\n",
        "    X_others = np.delete(X, j, axis=1)   # predictors (all items except j)\n",
        "    y = X[:, j]                         # target vector for item j\n",
        "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, fit_intercept=False,\n",
        "                        positive=True, max_iter=max_iter, tol=tol)\n",
        "    model.fit(X_others, y, sample_weight=sample_weights)\n",
        "    coefs = model.coef_\n",
        "    n_items = X.shape[1]\n",
        "    w_j = np.zeros(n_items)\n",
        "    w_j[:j] = coefs[:j]\n",
        "    w_j[j] = 0  # enforce zero diagonal\n",
        "    w_j[j+1:] = coefs[j:]\n",
        "    return w_j\n",
        "\n",
        "def train_slim(user_track_matrix, user_gender_map, female_weight=1.0,\n",
        "                alpha=1e-3, l1_ratio=0.01, max_iter=1000, tol=1e-4):\n",
        "    \"\"\"\n",
        "    Trains a SLIM model using ElasticNet per item with gender-aware sample weighting.\n",
        "    This version parallelizes training over items.\n",
        "\n",
        "    Parameters:\n",
        "        user_track_matrix : DataFrame (rows: users, columns: items)\n",
        "        user_gender_map   : Dictionary mapping user_id to gender\n",
        "        female_weight     : Multiplier for female users' sample weights\n",
        "        alpha             : Regularization strength\n",
        "        l1_ratio          : Balance between L1 and L2 penalty\n",
        "        max_iter, tol     : Solver settings\n",
        "\n",
        "    Returns:\n",
        "        W : Learned SLIM weight matrix (n_items x n_items) with zeros on the diagonal.\n",
        "    \"\"\"\n",
        "    # Convert the interaction matrix to a dense NumPy array.\n",
        "    X = user_track_matrix.values.astype(np.float32)\n",
        "    n_users, n_items = X.shape\n",
        "\n",
        "    # Build sample weights: female users receive a higher weight.\n",
        "    sample_weights = np.array([\n",
        "        female_weight if user_gender_map.get(user, 'male') == 'female' else 1.0\n",
        "        for user in user_track_matrix.index\n",
        "    ])\n",
        "\n",
        "    # Parallelize training over each item (each column in X).\n",
        "    with tqdm_joblib(tqdm(desc=\"Training items\", total=n_items)) as progress_bar:\n",
        "        results = Parallel(n_jobs=-1)(\n",
        "            delayed(train_item)(j, X, sample_weights, alpha, l1_ratio, max_iter, tol)\n",
        "            for j in range(n_items)\n",
        "        )\n",
        "\n",
        "    # Assemble the individual weight vectors into the weight matrix (columns correspond to items).\n",
        "    W = np.column_stack(results)\n",
        "    return W"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70855dab",
      "metadata": {
        "id": "70855dab"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54698e88",
      "metadata": {
        "id": "54698e88"
      },
      "outputs": [],
      "source": [
        "def get_recommendations_for_user_slim(W, user_id, user_track_matrix, track_list, top_n=10):\n",
        "    \"\"\"\n",
        "    For a given user, generate recommendations using the SLIM weight matrix.\n",
        "    The prediction scores for a user are computed as:\n",
        "        scores = user_vector dot W\n",
        "    Items the user already interacted with are excluded from the recommendation list.\n",
        "\n",
        "    Parameters:\n",
        "        W                : SLIM weight matrix (n_items x n_items)\n",
        "        user_id          : user identifier\n",
        "        user_track_matrix: pandas DataFrame (users x items)\n",
        "        track_list       : list of track IDs corresponding to the columns of user_track_matrix/W\n",
        "        top_n            : number of recommendations to return\n",
        "\n",
        "    Returns:\n",
        "        recommended_items: list of recommended track IDs\n",
        "    \"\"\"\n",
        "    if user_id not in user_track_matrix.index:\n",
        "        return []\n",
        "\n",
        "    # Get the binary interaction vector for the user.\n",
        "    user_vector = user_track_matrix.loc[user_id].values.astype(float)   # shape: (n_items,)\n",
        "    scores = np.dot(user_vector, W)  # predicted scores for each item\n",
        "\n",
        "    # Exclude items already seen by setting their score to -infinity.\n",
        "    user_history = set(user_track_matrix.loc[user_id][user_track_matrix.loc[user_id] > 0].index)\n",
        "    for idx, track in enumerate(track_list):\n",
        "        if track in user_history:\n",
        "            scores[idx] = -np.inf\n",
        "\n",
        "    # Get the indices of the top_n scores.\n",
        "    recommended_indices = np.argsort(scores)[::-1][:top_n]\n",
        "    recommended_tracks = [track_list[i] for i in recommended_indices if scores[i] != -np.inf]\n",
        "\n",
        "    return recommended_tracks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bf1e7f4",
      "metadata": {
        "id": "5bf1e7f4"
      },
      "source": [
        "# Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2da381db",
      "metadata": {
        "id": "2da381db"
      },
      "outputs": [],
      "source": [
        "def ndcg_at_k(relevances, k):\n",
        "    \"\"\"\n",
        "    Compute NDCG@k given a list of binary relevance scores.\n",
        "    \"\"\"\n",
        "    relevances = np.asarray(relevances, dtype=np.float64)[:k]\n",
        "    if relevances.size == 0:\n",
        "        return 0.0\n",
        "    discounts = np.log2(np.arange(2, relevances.size + 2))\n",
        "    dcg = np.sum(relevances / discounts)\n",
        "    ideal_relevances = np.sort(relevances)[::-1]\n",
        "    idcg = np.sum(ideal_relevances / discounts)\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "def evaluate_ndcg_slim(W, df, holdout_df, user_track_matrix, track_list, top_n=10):\n",
        "    \"\"\"\n",
        "    Evaluate recommendations using NDCG@k for each user in a holdout set.\n",
        "\n",
        "    Parameters:\n",
        "        W                : SLIM weight matrix.\n",
        "        df               : Original DataFrame (assumes a 'gender' column exists).\n",
        "        holdout_df       : DataFrame with ground-truth interactions.\n",
        "        user_track_matrix: Training user-item matrix.\n",
        "        track_list       : List of track IDs.\n",
        "        top_n            : Number of recommendations.\n",
        "\n",
        "    Returns:\n",
        "        overall_ndcg   : Average NDCG@top_n over all users.\n",
        "        ndcg_by_gender : Dictionary of average NDCG scores split by gender.\n",
        "    \"\"\"\n",
        "    # Map each user to his/her ground-truth holdout items.\n",
        "    user_holdout = holdout_df.groupby('user_id')['track_id'].apply(set).to_dict()\n",
        "    # Extract user attributes (like gender).\n",
        "    user_gender = df.set_index('user_id')['gender'].to_dict()\n",
        "\n",
        "    ndcg_scores = {}\n",
        "    ndcg_by_gender = {}\n",
        "\n",
        "    for user, true_items in user_holdout.items():\n",
        "        recs = get_recommendations_for_user_slim(W, user, user_track_matrix, track_list, top_n=top_n)\n",
        "        relevances = [1 if rec in true_items else 0 for rec in recs]\n",
        "        ndcg = ndcg_at_k(relevances, top_n)\n",
        "        ndcg_scores[user] = ndcg\n",
        "\n",
        "        gender = user_gender.get(user, 'unknown')\n",
        "        ndcg_by_gender.setdefault(gender, []).append(ndcg)\n",
        "\n",
        "    overall_ndcg = np.mean(list(ndcg_scores.values())) if ndcg_scores else 0.0\n",
        "    avg_ndcg_by_gender = {g: np.mean(scores) for g, scores in ndcg_by_gender.items()}\n",
        "\n",
        "    print(\"\\nSet Evaluation:\")\n",
        "    print(f\"Overall NDCG@{top_n}: {overall_ndcg:.4f}\")\n",
        "    print(\"NDCG by gender:\", avg_ndcg_by_gender)\n",
        "\n",
        "    return overall_ndcg, avg_ndcg_by_gender\n",
        "\n",
        "def grid_search_validation_slim(user_track_matrix, track_list, df, val_holdout_df,\n",
        "                                        user_gender_map, female_weight, candidate_alphas,\n",
        "                                        fixed_l1_ratio=0.01, top_n=10):\n",
        "    \"\"\"\n",
        "    Performs grid search over candidate alpha values.\n",
        "\n",
        "    For each alpha, the SLIM model is trained and evaluated on the validation holdout set using NDCG.\n",
        "    Returns:\n",
        "        best_alpha  : The alpha value that produced the highest overall NDCG.\n",
        "        best_ndcg   : The best NDCG value.\n",
        "        grid_results: A list of tuples (alpha, overall_ndcg).\n",
        "    \"\"\"\n",
        "    best_ndcg = -1.0\n",
        "    best_alpha = None\n",
        "    grid_results = []\n",
        "\n",
        "    for alpha in candidate_alphas:\n",
        "        W_candidate = train_slim(user_track_matrix, user_gender_map, female_weight=female_weight,\n",
        "                                    alpha=alpha, l1_ratio=fixed_l1_ratio)\n",
        "        overall_ndcg_val, _ = evaluate_ndcg_slim(W_candidate, df, val_holdout_df, user_track_matrix, track_list, top_n=top_n)\n",
        "        grid_results.append((alpha, overall_ndcg_val))\n",
        "        print(f\"alpha: {alpha} => NDCG: {overall_ndcg_val:.4f}\")\n",
        "        if overall_ndcg_val > best_ndcg:\n",
        "            best_ndcg = overall_ndcg_val\n",
        "            best_alpha = alpha\n",
        "    print(\"\\nBest alpha:\", best_alpha)\n",
        "    print(\"Best overall NDCG on validation set:\", best_ndcg)\n",
        "    return best_alpha, best_ndcg, grid_results\n",
        "\n",
        "\n",
        "def compute_diversity_for_list(recommended_tracks, user_track_matrix, track_list, W):\n",
        "    \"\"\"\n",
        "    Compute intra-list diversity: average dissimilarity among all pairs of recommended tracks.\n",
        "    Dissimilarity is defined as 1 - cosine similarity. Here we retrieve the\n",
        "    item vectors (columns in the training matrix) for the recommended tracks.\n",
        "\n",
        "    Parameters:\n",
        "        recommended_tracks: list of track IDs.\n",
        "        user_track_matrix: training user-item matrix.\n",
        "        track_list       : list of track IDs.\n",
        "        W                : weight matrix is not used directly here but one can also compute\n",
        "                            diversity on the basis of the original item vectors.\n",
        "\n",
        "    Returns:\n",
        "        diversity score (float)\n",
        "    \"\"\"\n",
        "    if len(recommended_tracks) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    # For diversity we use the binary interaction vectors (or you could precompute item feature vectors)\n",
        "    indices = [track_list.index(t) for t in recommended_tracks if t in track_list]\n",
        "    # Extract the corresponding columns from the training matrix as item profiles.\n",
        "    # Transpose the matrix so each row is an item vector.\n",
        "    item_matrix = user_track_matrix.values.T\n",
        "    vectors = item_matrix[indices]\n",
        "\n",
        "    sim_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Compute average pairwise similarity (ignoring the diagonal).\n",
        "    sum_similarity = 0.0\n",
        "    count = 0\n",
        "    n = len(indices)\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            sum_similarity += sim_matrix[i, j]\n",
        "            count += 1\n",
        "\n",
        "    avg_similarity = sum_similarity / count if count > 0 else 0.0\n",
        "    return 1 - avg_similarity\n",
        "\n",
        "def evaluate_metrics_slim(W, df, holdout_df, user_track_matrix, track_list, top_n=10):\n",
        "    \"\"\"\n",
        "    Evaluate recommendation metrics (Recall, Coverage, Diversity) for the holdout set.\n",
        "    Also compute per-gender metrics using the user attributes from df.\n",
        "\n",
        "    Parameters:\n",
        "        W                : SLIM weight matrix.\n",
        "        holdout_df       : DataFrame with ground-truth holdout interactions.\n",
        "        user_track_matrix: training user-item matrix.\n",
        "        track_list       : list of track IDs.\n",
        "        df               : original DataFrame (assumes a 'gender' column).\n",
        "        top_n            : number of recommendations.\n",
        "\n",
        "    Returns:\n",
        "        overall_recall, overall_coverage, overall_diversity, and a dictionary of per-gender metrics.\n",
        "    \"\"\"\n",
        "    # Ground truth mapping.\n",
        "    user_holdout = holdout_df.groupby('user_id')['track_id'].apply(set).to_dict()\n",
        "    user_gender = df.set_index('user_id')['gender'].to_dict()\n",
        "\n",
        "    recall_scores = {}\n",
        "    diversity_scores = {}\n",
        "    coverage_by_gender = {}\n",
        "\n",
        "    for user, true_items in user_holdout.items():\n",
        "        recs = get_recommendations_for_user_slim(W, user, user_track_matrix, track_list, top_n=top_n)\n",
        "        # Recall@top_n computation.\n",
        "        recall = len(set(recs).intersection(true_items)) / len(true_items) if true_items else 0.0\n",
        "        recall_scores[user] = recall\n",
        "\n",
        "        # Diversity computation.\n",
        "        diversity = compute_diversity_for_list(recs, user_track_matrix, track_list, W)\n",
        "        diversity_scores[user] = diversity\n",
        "\n",
        "        gender = user_gender.get(user, 'unknown')\n",
        "        coverage_by_gender.setdefault(gender, set()).update(recs)\n",
        "\n",
        "    overall_recall = np.mean(list(recall_scores.values()))\n",
        "    overall_diversity = np.mean(list(diversity_scores.values()))\n",
        "    overall_coverage = len(set().union(*(recs for recs in coverage_by_gender.values()))) / len(track_list)\n",
        "\n",
        "    # Average per-gender metrics.\n",
        "    recall_by_gender = {}\n",
        "    diversity_by_gender = {}\n",
        "    coverage_metrics_by_gender = {}\n",
        "\n",
        "    for user, rec in recall_scores.items():\n",
        "        gender = user_gender.get(user, 'unknown')\n",
        "        recall_by_gender.setdefault(gender, []).append(rec)\n",
        "    for user, div in diversity_scores.items():\n",
        "        gender = user_gender.get(user, 'unknown')\n",
        "        diversity_by_gender.setdefault(gender, []).append(div)\n",
        "    for gender, rec_set in coverage_by_gender.items():\n",
        "        coverage_metrics_by_gender[gender] = len(rec_set) / len(track_list)\n",
        "\n",
        "    avg_recall_by_gender = {g: np.mean(scores) for g, scores in recall_by_gender.items()}\n",
        "    avg_diversity_by_gender = {g: np.mean(scores) for g, scores in diversity_by_gender.items()}\n",
        "\n",
        "    print(\"\\nEvaluation Metrics @ {}:\".format(top_n))\n",
        "    print(\"Overall Recall: {:.4f}\".format(overall_recall))\n",
        "    print(\"Recall by gender:\", avg_recall_by_gender)\n",
        "    print(\"\\nOverall Coverage: {:.4f}\".format(overall_coverage))\n",
        "    print(\"Coverage by gender:\", coverage_metrics_by_gender)\n",
        "    print(\"\\nOverall Diversity: {:.4f}\".format(overall_diversity))\n",
        "    print(\"Diversity by gender:\", avg_diversity_by_gender)\n",
        "\n",
        "    gender_metrics = {\n",
        "        'recall': avg_recall_by_gender,\n",
        "        'coverage': coverage_metrics_by_gender,\n",
        "        'diversity': avg_diversity_by_gender\n",
        "    }\n",
        "\n",
        "    return overall_recall, overall_coverage, overall_diversity, gender_metrics\n",
        "\n",
        "# Assuming recGap_CF_results, compute_recGap, and compute_compounding_factor are defined elsewhere:\n",
        "def recGap_CF_results(df, gender_metrics):\n",
        "    for key, value in gender_metrics.items():\n",
        "        print(f\"\\nFor the {key} metric\")\n",
        "        compute_recGap(value)\n",
        "        compute_compounding_factor(df, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69baa079",
      "metadata": {
        "id": "69baa079"
      },
      "source": [
        "# Main Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df10e685",
      "metadata": {
        "id": "df10e685"
      },
      "outputs": [],
      "source": [
        "def Evaluate_SLIM(user_track_matrix, sparse_item_matrix, track_list, df, df_val_holdout, df_test_holdout,\n",
        "                    user_gender_map, female_weight=1.0, top_n=10):\n",
        "\n",
        "    # # Define candidate alpha values.\n",
        "    # candidate_alphas = [1e-3, 1e-2, 1e-1]\n",
        "    best_alpha = 1e-3\n",
        "\n",
        "    # best_alpha, best_ndcg, grid_results = grid_search_validation_slim(\n",
        "    #     user_track_matrix, track_list, df, df_val_holdout,\n",
        "    #     user_gender_map, female_weight, candidate_alphas, fixed_l1_ratio=0.01, top_n=top_n)\n",
        "\n",
        "    # Retrain final SLIM model with best alpha.\n",
        "    W = train_slim(user_track_matrix, user_gender_map, female_weight=female_weight,\n",
        "                    alpha=best_alpha, l1_ratio=0.01)\n",
        "\n",
        "    overall_ndcg_test, ndcg_by_gender_test = evaluate_ndcg_slim(W, df, df_test_holdout, user_track_matrix, track_list, top_n=top_n)\n",
        "    overall_recall, overall_coverage, overall_diversity, gender_metrics = evaluate_metrics_slim(W, df, df_test_holdout, user_track_matrix, track_list, top_n=top_n)\n",
        "    gender_metrics['ndcg'] = ndcg_by_gender_test\n",
        "    print(\"\\nOverall gender metrics:\", gender_metrics)\n",
        "    recGap_CF_results(df, gender_metrics)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def build_and_evaluate_slim(df, female_weight=1.0):\n",
        "    # Prepare the data.\n",
        "    df_model_train, df_val_holdout, df_test_holdout = build_data(df)\n",
        "    user_track_matrix, sparse_item_matrix, track_list = create_user_track_matrix(df_model_train)\n",
        "\n",
        "    # Create a mapping from user_id to gender (using the original dataframe).\n",
        "    user_gender_map = df.set_index('user_id')['gender'].to_dict()\n",
        "\n",
        "    # Train the SLIM model.\n",
        "    W = train_slim(user_track_matrix, user_gender_map, female_weight=female_weight, alpha=1e-3, l1_ratio=0.01)\n",
        "\n",
        "    # Evaluate the SLIM model.\n",
        "    Evaluate_SLIM(user_track_matrix, sparse_item_matrix, track_list, df, df_val_holdout, df_test_holdout,\n",
        "                user_gender_map, female_weight=female_weight, top_n=10)\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aab311f",
      "metadata": {
        "id": "5aab311f"
      },
      "source": [
        "# Running the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05de1538",
      "metadata": {
        "id": "05de1538"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/LFM-1b-DemoBiasSub-10k.csv', header=0)\n",
        "df_SMOTE = pd.read_csv('data/LFM-1b-DemoBiasSub-10k-SMOTE.csv', header=0)\n",
        "df_resampled = pd.read_csv('data/LFM-1b-DemoBiasSub-10k-Resampled.csv', header=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "750ade7f",
      "metadata": {
        "id": "750ade7f"
      },
      "outputs": [],
      "source": [
        "build_and_evaluate_slim(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d2e8f9a",
      "metadata": {
        "id": "7d2e8f9a"
      },
      "outputs": [],
      "source": [
        "g_counts = df['gender'].value_counts()\n",
        "total_users = len(df)\n",
        "p = np.array([g_count / total_users for g_count in g_counts])\n",
        "female_weight = p[0] / p[1]\n",
        "print(f\"weight adjustment = {female_weight:.2f}\")\n",
        "\n",
        "build_and_evaluate_slim(df, female_weight=female_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8645df40",
      "metadata": {
        "id": "8645df40"
      },
      "outputs": [],
      "source": [
        "build_and_evaluate_slim(df_SMOTE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a7eda9",
      "metadata": {
        "id": "77a7eda9"
      },
      "outputs": [],
      "source": [
        "build_and_evaluate_slim(df_resampled)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "cadc9208",
        "771cf053",
        "70855dab",
        "5bf1e7f4",
        "69baa079"
      ],
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
