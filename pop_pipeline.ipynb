{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Data_Splitter import build_data, create_user_track_matrix\n",
    "from Fairness_Metrics import compute_recGap, compute_compounding_factor\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Functions (Popularity-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_track_popularity(user_track_matrix):\n",
    "    \"\"\"\n",
    "    Compute a popularity score for each track by summing user interactions.\n",
    "    Returns a Series sorted by descending popularity.\n",
    "    \"\"\"\n",
    "    pop_series = user_track_matrix.sum(axis=0).sort_values(ascending=False)\n",
    "    return pop_series\n",
    "\n",
    "\n",
    "def get_recommendations_for_user_pop(user_id, user_track_matrix, track_popularity, top_n=10):\n",
    "    \"\"\"\n",
    "    Recommend the top-N most popular tracks (global popularity) that the user has not interacted with.\n",
    "    \"\"\"\n",
    "    if user_id not in user_track_matrix.index:\n",
    "        return []\n",
    "    \n",
    "    # Tracks that the user already interacted with.\n",
    "    user_history = set(user_track_matrix.loc[user_id][lambda row: row == 1].index)\n",
    "    \n",
    "    recommendations = []\n",
    "    # Iterate over tracks in order of popularity.\n",
    "    for track in track_popularity.index:\n",
    "        if track not in user_history:\n",
    "            recommendations.append(track)\n",
    "        if len(recommendations) >= top_n:\n",
    "            break\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(relevances, k):\n",
    "    \"\"\"\n",
    "    Compute NDCG@k given a list of binary relevance scores.\n",
    "    \"\"\"\n",
    "    relevances = np.asfarray(relevances)[:k]\n",
    "    if relevances.size == 0:\n",
    "        return 0.0\n",
    "    discounts = np.log2(np.arange(2, relevances.size + 2))\n",
    "    dcg = np.sum(relevances / discounts)\n",
    "    ideal_relevances = np.sort(relevances)[::-1]\n",
    "    idcg = np.sum(ideal_relevances / discounts)\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def evaluate_ndcg_pop(df, holdout_df, user_track_matrix, track_popularity, top_n=10):\n",
    "    \"\"\"\n",
    "    Evaluate NDCG@k for the popularity recommender on a holdout set.\n",
    "    Returns overall NDCG and NDCG by gender.\n",
    "    \"\"\"\n",
    "    # Create mapping from user_id to their holdout (ground truth) track_ids.\n",
    "    user_holdout = holdout_df.groupby('user_id')['track_id'].apply(set).to_dict()\n",
    "    user_gender = df.set_index('user_id')['gender'].to_dict()\n",
    "\n",
    "    ndcg_scores = {}    # per user scores\n",
    "    ndcg_by_gender = {} # aggregated scores per gender\n",
    "\n",
    "    for user, true_items in user_holdout.items():\n",
    "        recs = get_recommendations_for_user_pop(user, user_track_matrix, track_popularity, top_n=top_n)\n",
    "        relevances = [1 if rec in true_items else 0 for rec in recs]\n",
    "        ndcg = ndcg_at_k(relevances, top_n)\n",
    "        ndcg_scores[user] = ndcg\n",
    "\n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        ndcg_by_gender.setdefault(gender, []).append(ndcg)\n",
    "\n",
    "    overall_ndcg = np.mean(list(ndcg_scores.values())) if ndcg_scores else 0.0\n",
    "    avg_ndcg_by_gender = {gender: np.mean(scores) for gender, scores in ndcg_by_gender.items()}\n",
    "\n",
    "    print(\"\\nSet Evaluation:\")\n",
    "    print(f\"Overall NDCG@{top_n}: {overall_ndcg:.4f}\")\n",
    "    print(\"NDCG by gender:\", avg_ndcg_by_gender)\n",
    "\n",
    "    return overall_ndcg, avg_ndcg_by_gender\n",
    "\n",
    "def evaluate_metrics_pop(df, holdout_df, user_track_matrix, track_popularity, sparse_item_matrix, track_list, top_n=10):\n",
    "    \"\"\"\n",
    "    Evaluate recommendations on a holdout set using Recall, Coverage, and Diversity metrics.\n",
    "    \"\"\"\n",
    "    user_holdout = holdout_df.groupby('user_id')['track_id'].apply(set).to_dict()\n",
    "    user_gender = df.set_index('user_id')['gender'].to_dict()\n",
    "    \n",
    "    recall_scores = {}\n",
    "    diversity_scores = {}\n",
    "    coverage_by_gender = {}  # For coverage per gender.\n",
    "    \n",
    "    for user, true_items in user_holdout.items():\n",
    "        recs = get_recommendations_for_user_pop(user, user_track_matrix, track_popularity, top_n=top_n)\n",
    "        \n",
    "        # Compute Recall@top_n\n",
    "        recall = len(set(recs).intersection(true_items)) / len(true_items) if true_items else 0.0\n",
    "        recall_scores[user] = recall\n",
    "        \n",
    "        # For Diversity, we still use the sparse item representations and cosine similarity.\n",
    "        diversity = compute_diversity_for_list(recs, sparse_item_matrix, track_list)\n",
    "        diversity_scores[user] = diversity\n",
    "        \n",
    "        # Collect recommended items per gender.\n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        coverage_by_gender.setdefault(gender, set()).update(recs)\n",
    "    \n",
    "    overall_recall = np.mean(list(recall_scores.values()))\n",
    "    overall_diversity = np.mean(list(diversity_scores.values()))\n",
    "    overall_coverage = len(set().union(*(recs for recs in coverage_by_gender.values()))) / len(track_popularity)\n",
    "    \n",
    "    recall_by_gender = {}\n",
    "    diversity_by_gender = {}\n",
    "    coverage_metrics_by_gender = {}\n",
    "    \n",
    "    # Organize per-user metrics by gender.\n",
    "    for user, rec in recall_scores.items():\n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        recall_by_gender.setdefault(gender, []).append(rec)\n",
    "    \n",
    "    for user, div in diversity_scores.items():\n",
    "        gender = user_gender.get(user, 'unknown')\n",
    "        diversity_by_gender.setdefault(gender, []).append(div)\n",
    "    \n",
    "    for gender, rec_set in coverage_by_gender.items():\n",
    "        coverage_metrics_by_gender[gender] = len(rec_set) / len(track_popularity)\n",
    "    \n",
    "    avg_recall_by_gender = {g: np.mean(scores) for g, scores in recall_by_gender.items()}\n",
    "    avg_diversity_by_gender = {g: np.mean(scores) for g, scores in diversity_by_gender.items()}\n",
    "\n",
    "    print(\"\\nEvaluation Metrics @ {}:\".format(top_n))\n",
    "    print(\"\\nOverall Recall: {:.4f}\".format(overall_recall))\n",
    "    print(\"Recall by gender:\", avg_recall_by_gender)\n",
    "\n",
    "    print(\"\\nOverall Coverage: {:.4f}\".format(overall_coverage))\n",
    "    print(\"Coverage by gender:\", coverage_metrics_by_gender)\n",
    "\n",
    "    print(\"\\nOverall Diversity: {:.4f}\".format(overall_diversity))\n",
    "    print(\"Diversity by gender:\", avg_diversity_by_gender)\n",
    "    \n",
    "    gender_metrics = {\n",
    "        'recall': avg_recall_by_gender,\n",
    "        'coverage': coverage_metrics_by_gender,\n",
    "        'diversity': avg_diversity_by_gender\n",
    "    }\n",
    "    \n",
    "    return overall_recall, overall_coverage, overall_diversity, gender_metrics\n",
    "\n",
    "def compute_diversity_for_list(recommended_tracks, sparse_item_matrix, track_list):\n",
    "    \"\"\"\n",
    "    Compute intra-list diversity as the average pairwise dissimilarity (1 - cosine similarity)\n",
    "    among recommended tracks.\n",
    "    \"\"\"\n",
    "    if len(recommended_tracks) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Get indices for the recommended tracks.\n",
    "    indices = [track_list.index(t) for t in recommended_tracks if t in track_list]\n",
    "    if not indices:\n",
    "        return 0.0\n",
    "\n",
    "    # Extract item vectors using the sparse matrix.\n",
    "    vectors = sparse_item_matrix[indices]\n",
    "    \n",
    "    # Compute pairwise cosine similarity.\n",
    "    sim_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    sum_similarity = 0.0\n",
    "    count = 0\n",
    "    n = len(indices)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            sum_similarity += sim_matrix[i, j]\n",
    "            count += 1\n",
    "    avg_similarity = sum_similarity / count if count > 0 else 0.0\n",
    "    return 1 - avg_similarity\n",
    "\n",
    "def recGap_CF_results(df, gender_metrics):\n",
    "    for key, value in gender_metrics.items():\n",
    "        print(f\"\\nFor the {key} metric\")\n",
    "        compute_recGap(value)\n",
    "        compute_compounding_factor(df, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate_pop(user_track_matrix, sparse_item_matrix, track_list, track_popularity, df, df_val_holdout, df_test_holdout):\n",
    "    # Evaluate NDCG on the validation and test holdout sets.\n",
    "    overall_ndcg_val, ndcg_by_gender_val = evaluate_ndcg_pop(df, df_val_holdout, user_track_matrix, track_popularity, top_n=10)\n",
    "    overall_ndcg_test, ndcg_by_gender_test = evaluate_ndcg_pop(df, df_test_holdout, user_track_matrix, track_popularity, top_n=10)\n",
    "    \n",
    "    # Evaluate other metrics on the test set.\n",
    "    overall_recall, overall_coverage, overall_diversity, gender_metrics = evaluate_metrics_pop(\n",
    "        df, df_test_holdout, user_track_matrix, track_popularity, sparse_item_matrix, track_list, top_n=10)\n",
    "    \n",
    "    gender_metrics['ndcg'] = ndcg_by_gender_test \n",
    "    print(\"\\nAggregated Gender Metrics:\", gender_metrics)\n",
    "    recGap_CF_results(df, gender_metrics)\n",
    "\n",
    "def build_and_evaluate_pop(df):\n",
    "    # Build training and holdout datasets.\n",
    "    df_model_train, df_val_holdout, df_test_holdout = build_data(df)\n",
    "    \n",
    "    # Create user–track interaction matrix and its sparse representation.\n",
    "    user_track_matrix, sparse_item_matrix, track_list = create_user_track_matrix(df_model_train)\n",
    "    \n",
    "    # Compute track popularity from the training data.\n",
    "    track_popularity = compute_track_popularity(user_track_matrix)\n",
    "    \n",
    "    # Evaluate the popularity-based recommender.\n",
    "    Evaluate_pop(user_track_matrix, sparse_item_matrix, track_list, track_popularity, df, df_val_holdout, df_test_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/LFM-1b-DemoBiasSub-10k.csv', header=0)\n",
    "df_SMOTE = pd.read_csv('data/LFM-1b-DemoBiasSub-10k-SMOTE.csv', header=0)\n",
    "df_resampled = pd.read_csv('data/LFM-1b-DemoBiasSub-10k-Resampled.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.0534\n",
      "NDCG by gender: {'m': 0.04757433285532539, 'f': 0.06948440699081211}\n",
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.0517\n",
      "NDCG by gender: {'m': 0.04676159557182249, 'f': 0.06512431843008354}\n",
      "\n",
      "Evaluation Metrics @ 10:\n",
      "\n",
      "Overall Recall: 0.0506\n",
      "Recall by gender: {'m': 0.04404866594119496, 'f': 0.06854091957992486}\n",
      "\n",
      "Overall Coverage: 0.0017\n",
      "Coverage by gender: {'m': 0.0016, 'f': 0.0017}\n",
      "\n",
      "Overall Diversity: 0.9269\n",
      "Diversity by gender: {'m': 0.9268554270548213, 'f': 0.9271297065656413}\n",
      "\n",
      "Aggregated Gender Metrics: {'recall': {'m': 0.04404866594119496, 'f': 0.06854091957992486}, 'coverage': {'m': 0.0016, 'f': 0.0017}, 'diversity': {'m': 0.9268554270548213, 'f': 0.9271297065656413}, 'ndcg': {'m': 0.04676159557182249, 'f': 0.06512431843008354}}\n",
      "\n",
      "For the recall metric\n",
      "RecGap Score: 0.0244922536387299 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.7575144921328422\n",
      "Females: 0.24248550786715783\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.6675144983401606\n",
      "Females: 0.33248550165983937\n",
      "\n",
      "Compounding Factor (KL Divergence): 0.019270018262573144\n",
      "\n",
      "For the coverage metric\n",
      "RecGap Score: 9.999999999999983e-05 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.7575144921328422\n",
      "Females: 0.24248550786715783\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.74620549104107\n",
      "Females: 0.25379450895893\n",
      "\n",
      "Compounding Factor (KL Divergence): 0.00034105708659656897\n",
      "\n",
      "For the diversity metric\n",
      "RecGap Score: 0.0002742795108200413 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.7575144921328422\n",
      "Females: 0.24248550786715783\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.7574601387047778\n",
      "Females: 0.24253986129522218\n",
      "\n",
      "Compounding Factor (KL Divergence): 8.040869892950273e-09\n",
      "\n",
      "For the ndcg metric\n",
      "RecGap Score: 0.01836272285826105 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.7575144921328422\n",
      "Females: 0.24248550786715783\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.6916543506484468\n",
      "Females: 0.30834564935155323\n",
      "\n",
      "Compounding Factor (KL Divergence): 0.010636455850900964\n"
     ]
    }
   ],
   "source": [
    "build_and_evaluate_pop(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.0409\n",
      "NDCG by gender: {'m': 0.05287491089839556, 'f': 0.011427286580498248}\n",
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.0337\n",
      "NDCG by gender: {'m': 0.04326116044305975, 'f': 0.009130876132999616}\n",
      "\n",
      "Evaluation Metrics @ 10:\n",
      "\n",
      "Overall Recall: 0.0342\n",
      "Recall by gender: {'m': 0.04589248731300294, 'f': 0.003937465273216622}\n",
      "\n",
      "Overall Coverage: 0.0018\n",
      "Coverage by gender: {'m': 0.0018, 'f': 0.0016}\n",
      "\n",
      "Overall Diversity: 0.9249\n",
      "Diversity by gender: {'m': 0.9248248019885202, 'f': 0.9249955221462198}\n",
      "\n",
      "Aggregated Gender Metrics: {'recall': {'m': 0.04589248731300294, 'f': 0.003937465273216622}, 'coverage': {'m': 0.0018, 'f': 0.0016}, 'diversity': {'m': 0.9248248019885202, 'f': 0.9249955221462198}, 'ndcg': {'m': 0.04326116044305975, 'f': 0.009130876132999616}}\n",
      "\n",
      "For the recall metric\n",
      "RecGap Score: 0.041955022039786315 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.5\n",
      "Females: 0.5\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.9209819582628797\n",
      "Females: 0.07901804173712028\n",
      "\n",
      "Compounding Factor (KL Divergence): 0.6170497736298637\n",
      "\n",
      "For the coverage metric\n",
      "RecGap Score: 0.00019999999999999987 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.5\n",
      "Females: 0.5\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.5294117647058822\n",
      "Females: 0.47058823529411764\n",
      "\n",
      "Compounding Factor (KL Divergence): 0.0017331039882432196\n",
      "\n",
      "For the diversity metric\n",
      "RecGap Score: 0.0001707201576995887 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.5\n",
      "Females: 0.5\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.4999538549351328\n",
      "Females: 0.5000461450648671\n",
      "\n",
      "Compounding Factor (KL Divergence): 4.2587340976117855e-09\n",
      "\n",
      "For the ndcg metric\n",
      "RecGap Score: 0.034130284310060134 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.5\n",
      "Females: 0.5\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.8257201527231338\n",
      "Females: 0.17427984727686624\n",
      "\n",
      "Compounding Factor (KL Divergence): 0.27614897697231444\n"
     ]
    }
   ],
   "source": [
    "build_and_evaluate_pop(df_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.0517\n",
      "NDCG by gender: {'m': 0.04838376903446011, 'f': 0.058016290029810666, nan: 0.04483920730746311}\n",
      "\n",
      "Set Evaluation:\n",
      "Overall NDCG@10: 0.0531\n",
      "NDCG by gender: {'m': 0.047272149416857055, 'f': 0.06072342054679076, nan: 0.13250983570755606}\n",
      "\n",
      "Evaluation Metrics @ 10:\n",
      "\n",
      "Overall Recall: 0.0500\n",
      "Recall by gender: {'m': 0.046319738373610427, 'f': 0.056149161193642334, nan: 0.06536078717201166}\n",
      "\n",
      "Overall Coverage: 0.0016\n",
      "Coverage by gender: {'m': 0.0016, 'f': 0.0016, nan: 0.0015}\n",
      "\n",
      "Overall Diversity: 0.9267\n",
      "Diversity by gender: {'m': 0.9266350563529345, 'f': 0.9268319943942741, nan: 0.9278840605144505}\n",
      "\n",
      "Aggregated Gender Metrics: {'recall': {'m': 0.046319738373610427, 'f': 0.056149161193642334, nan: 0.06536078717201166}, 'coverage': {'m': 0.0016, 'f': 0.0016, nan: 0.0015}, 'diversity': {'m': 0.9266350563529345, 'f': 0.9268319943942741, nan: 0.9278840605144505}, 'ndcg': {'m': 0.047272149416857055, 'f': 0.06072342054679076, nan: 0.13250983570755606}}\n",
      "\n",
      "For the recall metric\n",
      "RecGap Score: 0.009829422820031908 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.48437451498457995\n",
      "Females: 0.48437451498457995\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.45203704313433846\n",
      "Females: 0.5479629568656617\n",
      "\n",
      "Compounding Factor (KL Divergence): -0.026279749213085693\n",
      "\n",
      "For the coverage metric\n",
      "RecGap Score: 0.0 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.48437451498457995\n",
      "Females: 0.48437451498457995\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.5\n",
      "Females: 0.5\n",
      "\n",
      "Compounding Factor (KL Divergence): -0.030757490725387606\n",
      "\n",
      "For the diversity metric\n",
      "RecGap Score: 0.00019693804133957826 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.48437451498457995\n",
      "Females: 0.48437451498457995\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.499946873066543\n",
      "Females: 0.5000531269334569\n",
      "\n",
      "Compounding Factor (KL Divergence): -0.030757485256855312\n",
      "\n",
      "For the ndcg metric\n",
      "RecGap Score: 0.013451271129933703 \n",
      "\n",
      "Original data distribution:\n",
      "Males: 0.48437451498457995\n",
      "Females: 0.48437451498457995\n",
      "\n",
      "Metric score distribution:\n",
      "Males: 0.43772304209116397\n",
      "Females: 0.562276957908836\n",
      "\n",
      "Compounding Factor (KL Divergence): -0.02318416232403167\n"
     ]
    }
   ],
   "source": [
    "build_and_evaluate_pop(df_SMOTE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
